\documentclass[a4paper,10pt]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages for scientific writing
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref} % for smart cross-references

%% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%% Title information
\title{Project nº 1: Loan Approval Prediction with Linear and Bayesian Models\\
    Advanced Machine Learning (MDS)}

\author{Martin Tazón \and Daniel Weronski \and Martin de Peretti}

\date{Date: November 11, 2025}

\begin{document}
    
    \maketitle
    
    \begin{abstract}

%     GUIDANCE: The abstract should be a single paragraph (150-250 words) that concisely summarizes:

%     (1) The problem you are addressing and its motivation
We address a supervised binary classification problem: predicting loan approval from mixed tabular data containing both numerical and categorical features. Our objective is to evaluate three foundational methods from Part I of the course: a discriminative Generalized Linear Model (Logistic Regression), a Bayesian generative classifier (Naive Bayes), and a margin-based linear classifier (Linear SVM). \\

%     (2) The main methods/techniques you applied (from the corresponding course part)
% Do we care about the pipeline? Focus on the comparison of methods.
We construct a consistent and leakage-free preprocessing pipeline, including the removal of identifier variables, the encoding of categorical features, and the standardization of numerical attributes. All transformations are fitted exclusively on the training data within cross-validation folds to ensure methodological soundness. Performance will be evaluated using standard metrics for binary classification, including ROC-AUC, accuracy, and class-wise measures. \\

%     (3) The key results or findings
\textcolor{red}{TO DO (brief summary of empirical results and conclusions after completing the others parts) }
%     Write this LAST, after completing the rest of the report.
%     Example structure: "We address the problem of... using methods from Part [I/II/III] of the course, specifically... Our experimental results show that... We conclude that..." or ``we prove that ...''
        
    \end{abstract}
        
    \section{Introduction}
    \label{sec:introduction}

%    GUIDANCE: The introduction should provide context and motivation for your work. Include:
%     

%    (1) The problem you are addressing: what is it, why is it important or interesting?
Access to credit is one of the main factors influencing economic inclusion, yet loan approval decisions often rely on multiple socio-economic and financial criteria. \\ 

%    (2) Brief background: what makes this problem challenging?
The presented dataset offers a great example of how financial entities rate their customers based on the data they have from them. There are two variables in the dataset that have been fabricated for this process, "Credit Score", which is a financial risks score calculated from credit information by means of a standardized formula, and "Points". This last variable turns out to be the deciding factor used to determine the approval since it linearly separates the data in only one dimension. We will opt to use this variable or not depending on rather it has pedagogical meaning for the discused model. %NOT SURE ABOUT THIS%

%    (3) Your approach: what methods from the course are you applying? (mention Part I/II/III)
The dataset used in this work includes mostly numerical features, with only one ordinal features describing applicants’ profiles and their corresponding loan approval outcomes. 
Such data naturally involves uncertainty and feature correlations, making it suitable for exploring the concepts introduced in \emph{Part I} of the course, namely Bayesian reasoning, Generalized Linear Models (GLMs), and linear discriminant methods. For completeness we will fabricate a categorical feature by classifying the applicants by their work experience.\\

We focus on three complementary approaches:
\begin{itemize}
    \item the \textbf{Naive Bayes classifier}, which follows a Bayesian generative approach and assumes that features are conditionally independent given the target class;
    \item the \textbf{Logistic Regression model}, a Generalized Linear Model that directly estimates the probability of loan approval as a function of a linear combination of the features;
    \item the \textbf{Linear Support Vector Machine (SVM)}, which searches for a decision boundary that maximizes the margin between approved and rejected applicants.
\end{itemize}

These three methods represent different but complementary ways of learning from data. Comparing them within the same framework allows us to assess how these theoretical differences affect performance, interpretability, and robustness in a real-world classification task.\\

%    (4) Main contributions: what do you accomplish in this work?
The goal of this project is therefore to link theoretical understanding with practical application. 
We aim to analyze how each algorithm behaves on the same financial dataset, discuss their assumptions, and evaluate their ability to make reliable and interpretable predictions.\\

%    (5) Structure: briefly outline the rest of the document ("The rest of this report is organized as follows...")
The rest of this report is organized as follows. 
Section~\ref{sec:problem} formalizes the prediction problem. 
Section~\ref{sec:related} briefly reviews related works and previous approaches. % I would delte.
Section~\ref{sec:data} describes the dataset and preprocessing pipeline. 
Section~\ref{sec:methodology} presents the applied methods and experimental setup. 
Section~\ref{sec:discussion} discusses the results and insights. 
Finally, Section~\ref{sec:conclusions} summarizes the main findings and perspectives.

        
    \section{\textcolor{red}{Problem statement}}
    \label{sec:problem}
    % GUIDANCE: 
    % (1) Clearly define what you are trying to accomplish
    % (2) Use mathematical notation appropriately but concisely.
    % (3) Make sure notation is clear
    %[Formal problem statement here]


    
    \section{\textcolor{red}{Related Work}}
    \label{sec:related}
    % GUIDANCE:   
    % (1) Brief description of relevant previous work on this problem or dataset
    % (2) How your work relates to or differs from previous approaches
    % (3) What you adopt or improve upon from related work
    % DO NOT copy/paste from papers - synthesize and cite properly!
    % EXAMPLE ALGORITHM IN PSEUDOCODE (adapt or remove, use for all algorithms throughout the work)
    % DO NOT display trivial non-informative pseudocode and make it match the notation used in the text
    \begin{algorithm}
        \caption{Algorithm name}
        \label{alg:example}
        \begin{algorithmic}[1]
            \REQUIRE Input data $X$, parameters $\theta_0$
            \ENSURE Optimized parameters $\theta^*$
            \STATE Initialize $\theta \leftarrow \theta_0$
            \WHILE{not converged}
            \STATE Compute gradient $g \leftarrow \nabla_\theta L(\theta)$
            \STATE Update $\theta \leftarrow \theta - \alpha g$
            \ENDWHILE
            \RETURN $\theta$
        \end{algorithmic}
    \end{algorithm}
    
     %   [Your related work discussion here]



    
    \section{Data and Preprocessing}
    \label{sec:data}
    
    % GUIDANCE: Describe your data and all preprocessing steps, or omit this section entirely if not applicable. This section should be thorough because preprocessing significantly impacts results.
    This section describes the dataset used for the loan approval prediction task, as well as the preprocessing steps applied prior to model training. 
A careful understanding of the data and its preparation is essential to ensure the validity and interpretability of the results.

    
  \subsection{Data description}
\label{sec:data-description}

    % GUIDANCE: Describe the dataset(s) you are using, omit if not applicable.
    
%    Source: where did you obtain it? (cite properly)
%    Size: number of observations, features, classes (if classification)
%    Nature of features: continuous, categorical, mixed?
%    Domain: what does the data represent? What is the application context?
%    Known challenges: class imbalance, missing values, noise?
%    
%    A well-formatted table summarizing dataset characteristics is very useful here. \emph{See the project guide for more details}.
    
The dataset, entitled \emph{\href{https://www.kaggle.com/datasets/anishdevedward/loan-approval-dataset}{Loan Approval Dataset}}, was obtained from Kaggle and contains information about applicants and their corresponding loan approval status. Upon inspcting the details of the data we found it to be artificially generated using Faker Library.
It includes both numerical and categorical features that describe socio-economic and financial characteristics such as income, credit score, years employed, and loan amount. 
The target variable indicates whether a loan application was approved or not, making this a binary classification problem.\\

Two columns, \texttt{name} and \texttt{city}, were removed. 
The first one was unique for each applicant and therefore non-informative, while the second contained too few repetitions (maximum frequency of 4), making it irrelevant for the model. 
After removing these variables, the dataset was left with only continuous attributes.

No missing data or outliers were detected, which simplified preprocessing and ensured that no imputation or cleaning procedures were necessary, as one would expect from fabricated data. 


% Should update to new feature.
    \begin{table}[H]
        \centering
        \caption{Dataset characteristics after cleaning.}
        \label{tab:dataset}
        \begin{tabular}{@{}lll@{}}
            \toprule
            \textbf{Property} & \textbf{Variable} & \textbf{Value} \\
            \midrule
            Number of observations & $n$ & 2,000  \\
            Number of features & $d$ & 5 \\
            Feature types & continuous & Income / Credit score / Loan amount / Years employed / Points\\
            Number of classes & $C$ & 2 (Approved / Rejected) \\
            Class distribution & balanced & Approved : 56.05\% / Rejected : 43.95\% \\
            Missing values & percentage & 0\% \\
            Outliers detected & & None \\
            Source & & \href{https://www.kaggle.com/datasets/anishdevedward/loan-approval-dataset}{Kaggle} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
\subsection{Exploratory data analysis}
\label{sec:eda}

    % GUIDANCE: Describe your initial exploration of the data:
    
    % (3) Insights gained that inform preprocessing or modeling decisions
    %
    % IMPORTANT: Every figure must have a caption and be referenced and discussed in the text.
        
    % EXAMPLE FIGURE (remove this comment and adapt):

Before model development, a brief exploratory analysis was conducted to better understand the dataset’s structure and relationships between variables. 
The target variable (\texttt{loan\_approved}) is roughly balanced, which facilitates evaluation without the need for class reweighting or resampling techniques.\\

% (1) Visualization of key features or relationships
\paragraph{Feature distributions.}
The numerical features (\texttt{income}, \texttt{credit\_score}, \texttt{loan\_amount}, \texttt{years\_employed}, and \texttt{points}) were inspected to identify general trends and potential anomalies. 
Most variables exhibit mild right-skewness, which is typical of financial data (for instance, a small proportion of applicants with very high income or credit scores). 
No outliers or implausible values were observed.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/boxplotVariables.png}
    \caption{Distributions of numerical features by loan approval status. Applicants with higher income, credit score, and points are more likely to be approved.}
    \label{fig:boxplots}
\end{figure}

As shown in Figure~\ref{fig:boxplots}, loan approval is strongly associated with higher values of \texttt{income}, \texttt{credit\_score}, and \texttt{points}, confirming the relevance of financial stability indicators in credit assessment.
By contrast, \texttt{loan\_amount} and \texttt{years\_employed} display weaker class separation, suggesting they contribute less to the predictive task.\\
The feature \texttt{points} is special since the classes can be linearly separated by only looking at this variable, suggesting that this is an indicator computed by the financial entity to decide weather a loand should be accepted or not.

    % (2) Identification of patterns, outliers, correlations
\paragraph{Correlation analysis.}
To assess potential redundancy and feature interdependence, the Pearson correlation matrix was computed for all numerical variables (Figure~\ref{fig:corr}). 
The analysis reveals a strong positive correlation ($r \approx 0.74$) between \texttt{credit\_score} and \texttt{points}, and a moderate one ($r \approx 0.45$) between \texttt{income} and \texttt{points}. 
These relationships are expected given the shared financial nature of the indicators and it is consistent with  the theory of the \texttt{points} feature being a combination of the other metric to create a final de matric upon wich the entity takes decisions.
Importantly, the correlation values remain below critical thresholds, indicating no severe multicollinearity that would compromise model interpretability or stability.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/correlationMatrix.png}
    \caption{Correlation matrix of numerical features. A strong relation is observed between credit score and points.}
    \label{fig:corr}
\end{figure}

Overall, the exploratory analysis confirms that the dataset is clean, balanced, and well-suited for evaluating probabilistic and linear classification methods.
However, two decisions where taken at this stage:
\begin{enumerate}
    \item Since the inclussion of the feature \texttt{points} into the models may lead to perfect scores when those models can make use of the fact that the classes are linearly separable along this feature. The project aims to discuss the effect of leaving or excluding the vairalbe for each modeling approach.
    \item Given that the dataset lacks categorical variables, a new variable was included to showcase the ability of the studied machine learning models to account for this kind of variables. This new varible was a boolean representing those applicants with less than 10 years of experience.\\
\end{enumerate}
The insights gained here, particularly the correlation between key financial features, will be directly leveraged in the modeling phase to interpret each algorithm’s behavior and hopefully find out how the feature \texttt{points} was computed.


    \subsection{Preprocessing steps}
    \label{sec:preprocessing}
    
    % GUIDANCE: Document ALL preprocessing steps in detail. For each step, explain:
    
    % (1) WHAT you did
    % (2) WHY you did it (justify based on data characteristics or method requirements)
    % (3) HOW you did it (be specific about techniques used)
    %
    % Common preprocessing steps to address (as applicable):
    % - Missing value treatment: deletion, imputation (mean, median, model-based)? Justify choice.
    % - Outlier treatment: detection method? removal or transformation?
    % - Feature scaling/normalization: standardization, min-max? Why needed for your methods?
    % - Encoding categorical variables: one-hot, label encoding, target encoding? Why?
    % - Feature selection: filter, wrapper, embedded methods? Criteria used?
    % - Feature extraction: PCA, domain-specific transformations?
    % - Class imbalance: over/undersampling, class weights, posterior probabilities?




The preprocessing stage aimed to ensure data consistency and compliance with the theoretical assumptions of the three models under study: Naive Bayes, Logistic Regression, and Linear SVM. Although the dataset was already clean, a few key steps were necessary to prepare it for modeling.\\

The variables \texttt{name} and \texttt{city} were first removed. The former uniquely identifies each applicant and carries no predictive information, while the latter exhibits very low repetition (maximum frequency of four). Retaining these variables could introduce unnecessary noise and reduce the generalization capacity of the models. After this step, all remaining features were numerical, simplifying subsequent processing.\\

A systematic verification confirmed that the dataset contained no missing or incoherent values. This is relevant since models such as Logistic Regression and SVM do not natively handle missing data. Visual inspection via boxplots and histograms revealed no outliers: all features fall within plausible financial ranges, indicating that no trimming or imputation was required.\\

Correlation analysis (see Figure~\ref{fig:corr}) showed limited multicollinearity among most variables, except for a strong correlation ($r = 0.74$) between \texttt{credit\_score} and \texttt{points}. This redundancy was deliberately retained to observe how each learning paradigm behaves when predictors are dependent: it challenges the conditional independence assumption of Naive Bayes, may slightly affect coefficient interpretability in Logistic Regression, and has minimal impact on the Linear SVM’s separating margin. Preserving both features therefore enriches the comparative analysis between generative and discriminative methods.\\

Since all features were numeric but on different scales (e.g., \texttt{income} in thousands versus \texttt{credit\_score} in hundreds), a normalization step was necessary. Standardization was performed using \texttt{StandardScaler} from \texttt{scikit-learn}, transforming each feature $x_i$ as:
\[
x_i' = \frac{x_i - \mu_i}{\sigma_i}
\]
where $\mu_i$ and $\sigma_i$ are the mean and standard deviation estimated on the training set. This transformation centers the data around zero with unit variance, preventing large-scale features from dominating the optimization. It stabilizes learning for Logistic Regression and SVM and aligns feature variances with the probabilistic assumptions of Gaussian Naive Bayes.\\

To evaluate the effect of scaling, we visualized feature distributions before and after standardization (Figure~\ref{fig:scaling}). For instance, \texttt{income} originally exhibited a large range and mild right-skewness typical of financial data. After standardization, its distribution is re-centered and rescaled, preserving its overall shape and relative ordering. This confirms that the transformation harmonizes feature magnitudes without distorting their underlying structure, ensuring that all dimensions contribute comparably to model training.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/scaling.png}
    \caption{Effect of standardization on two numerical features. 
    The distributions are centered around zero and rescaled to unit variance, 
    preserving the shape of the data while ensuring comparability across features.}
    \label{fig:scaling}
\end{figure}

Lastly, the new metric \texttt{plus\_10y\_exp} was created by assigning 0 to instances where $years\_employed<10$ and 1 otherwise.\\

Overall, preprocessing primarily involved removing redundant variables and applying standardization. These steps yielded a consistent and theoretically sound dataset suitable for rigorous comparison across the three selected learning algorithms.


    \section{Methodology}
    \label{sec:methodology}
    
    % GUIDANCE: This is typically the longest section (2-3 pages). Describe all methods you applied
    % from the corresponding course part. For each method:
    
    % (1) Brief description: what is the method and its key properties?
    % (2) Why chosen: justify based on problem characteristics and theoretical properties
    % (3) Mathematical formulation: present the key equations precisely
    % (4) Implementation details: hyperparameters, optimization algorithm, convergence criteria
    % (5) Theoretical properties: discuss relevant theoretical results (e.g., universal approximation,
    %     consistency, convergence guarantees, computational complexity)
    %
    % Balance completeness with conciseness - you don't need to re-derive results from class, but you must demonstrate understanding.
    
    \subsection{Experimental protocol}
    \label{sec:protocol}
    
    % GUIDANCE: Describe your overall experimental setup BEFORE describing individual methods:
    %
    % (1) Data splitting: training/validation/test split? Cross-validation? Ratios?
    % (2) Performance metrics: which metrics and why? (accuracy, F1, AUC, RMSE, etc.)
    % (3) Hyperparameter tuning: method used (grid search, random search)? Search space?
    % (4) Statistical significance: how do you assess if differences are significant?
    % (5) Computational environment: hardware, software versions (for reproducibility)
    %
    % CRITICAL (resampling): Explain how you avoid data leakage (preprocessing fit on training set only,
    % applied to validation/test sets). This is essential for methodological rigor.


All experiments follow a rigorous and reproducible evaluation protocol designed to ensure fair comparison between the three classifiers studied in this work: Logistic Regression, Gaussian Naive Bayes, and Linear SVM. The dataset was first split into a training set (80\%) and a test set (20\%) using a fixed random seed, ensuring that all models are trained and evaluated on identical data partitions. To prevent data leakage, all preprocessing steps involving the computation of statistics, specifically feature standardization, were fitted exclusively on the training set and subsequently applied to the test set.\\

Model selection and hyperparameter tuning were performed through cross-validation on the training set. We used a 5-fold cross-validation strategy, in which the training data is partitioned into five equally sized folds: four folds are used for model fitting, and the remaining fold serves as a validation set. This process is repeated five times so that each fold is used once for validation. Cross-validation provides an unbiased estimate of generalization performance and reduces the variance associated with a single train/validation split.\\

For Logistic Regression and Linear SVM, the regularization parameter $C$ was tuned over a logarithmic grid in order to identify the best trade-off between model flexibility and generalization. For Gaussian Naive Bayes, the hyperparameter \texttt{var\_smoothing} was briefly examined across several orders of magnitude; as performance remained essentially unchanged, the default value was retained.\\

All models were evaluated using accuracy, precision, recall, and F1-score. These metrics are well suited for a balanced binary classification problem and allow a fair comparison between the generative (Naive Bayes) and discriminative (Logistic Regression, SVM) approaches.\\


All results reported in the next section correspond to performance on the held-out test set, which was not used at any stage of model selection. Experiments were conducted using Python 3.11 and \texttt{scikit-learn} 1.4, ensuring full reproducibility of the experimental environment.\\

    
    \subsection{GLM – Logistic Regression}
\label{sec:method1}

\subsubsection{Model formulation}

Logistic Regression is a Generalized Linear Model (GLM) designed for binary classification.  
Given input features $x \in \mathbb{R}^d$, the model assumes that the conditional probability of the positive class follows a logistic function:
\[
P(y = 1 \mid x; \theta) = \sigma(\theta^\top x)
\quad \text{with} \quad
\sigma(z)=\frac{1}{1+e^{-z}}.
\]

Let $\ell(\theta)$ denote the empirical logistic loss:
\[
\ell(\theta) = - \frac{1}{n} \sum_{i=1}^n 
\left( y_i \log \sigma(\theta^\top x_i)
+ (1-y_i)\log (1-\sigma(\theta^\top x_i)) \right).
\]

To prevent overfitting and improve numerical stability, an $\ell_2$ regularization term is added, leading to the following optimization problem:
\[
\hat{\theta}
=
\arg\min_{\theta \in \mathbb{R}^d}
\left\{
\ell(\theta) + 
\lambda \|\theta\|_2^2
\right\},
\]
where $\lambda = 1/C$ controls the strength of regularization (large $C$ = weak regularization).

The final classifier predicts:
\[
\hat{y} = 
\begin{cases}
1 & \text{if } \sigma(\theta^\top x) \geq 0.5, \\
0 & \text{otherwise}.
\end{cases}
\]


\subsubsection{Theoretical properties and justification}

Logistic Regression is a well-calibrated discriminative model that directly estimates $P(y \mid x)$ rather than modeling $P(x \mid y)$.  
It is particularly suitable for this dataset because:

\begin{itemize}
    \item all predictors are numerical and standardized, matching well the linear decision boundary assumption;
    \item the dataset size is moderate (2,000 observations), allowing fast and stable convex optimization;
    \item regularization mitigates multicollinearity effects, which is relevant given the strong correlation observed between \texttt{credit\_score} and \texttt{points};
    \item the model yields interpretable coefficients, enabling analysis of how features influence loan approval decisions.
\end{itemize}

The optimization problem is convex, guaranteeing the existence of a unique global optimum and robust convergence.  
Because of its probabilistic output and low variance, Logistic Regression is a strong baseline for binary financial decisions where interpretability and calibration are important.


\subsubsection{Implementation details}

The model was implemented using the \texttt{LogisticRegression} class from \texttt{scikit-learn}.  
All features were standardized prior to training.  
To select the regularization strength, the parameter $C$ was explored over a logarithmic grid:

\[
C \in \{10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^2\}.
\]

The solver \texttt{liblinear} was used due to the small number of features and its robustness on binary classification with $\ell_2$ penalty.

Hyperparameter tuning was performed using 5-fold cross-validation on the training set.  
No data leakage occurred: scaling was fitted exclusively on the training folds and applied to validation folds.

\begin{table}[ht]
    \centering
    \caption{Hyperparameters for Logistic Regression}
    \label{tab:hyper-logreg}
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Hyperparameter} & \textbf{Search space} & \textbf{Best value} \\
        \midrule
        Regularization strength $C$ & $\{10^{-3},10^{-2},10^{-1},1,10,10^2\}$ & 10^{-1} \\
        Penalty & $\{\ell_2\}$ & $\ell_2$ \\
        Solver & \texttt{liblinear}, \texttt{lbfgs} & \texttt{lbfgs} \\
        Max iterations & $\{100, 200, 500\}$ & 500 \\
        \bottomrule
    \end{tabular}
\end{table}

    \subsection{Naive Bayes Classifier}
    \label{sec:method2}

    This sections covers the use of the Naive Bayes Classifier when facing a binary classification problem. This modeling approach is unique compared to the others discussed in this project as it follows a genrative approach rather than a discriminative one.\\
    
    In broad terms one can think of discriminant models as trying to learn the separation between the classes by modeling $P(y|X)$, while on the other hand, generative algorithms try to learn $P(X|y)$ (and also $P(y)$). This implies that generative algorithms learn how the features are conditioned on the target class being 0 or 1.\\

    Naive Bayes are a fammily of supervised machine learining algorithms based on applying the Bayes' Theorem with the 'naive' assumption of conditional independance between every parir of features given the value of the target variable.\\ 
    
    These classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality at the cost of ignoring possible feature dependancies.\\

    Overall, Naive Bayes is known to be a decent and efficient classifier, however they are known to perform poorly as estimators, meaning that the probability outputs are not to be taken too seriously.\\
    
    \subsubsection{Model formulation} 
    
    Recall The Bayes Theorem:
    $$P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$
    
    The model aims to make a prediction on the class given the input data, wich can be written as:
    $$P(y|X)=P(y|x_1,...,x_n) = \frac{P(y)\cdot P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$

    The 'naive' assumption of conditional independence is expressed as:
    $$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_n) = P(x_i|y)$$
    And when applied into the Bayes Theorem $\forall i$ yelds:
    
    $$P(y|x_1,...,x_n)=\frac{P(y)\prod_{i=1}^n P(x_i|y)}{P(x_1,...,x_n)}$$

    Since $P(x_1,...,x_n)$ is constant given the input, we can simpligy and derive the following classification rule:
    $$ P(y|x,...,x_n) \propto P(y)\prod_{i=1}^nP(x_i|y) \implies \hat{y} = argmax_yP(y)\prod_{i=1}^nP(x_i|y) $$

    One can use Maximum A Posteriori estimation to estimate $P(x_i|y)$ and $P(y)$, where the later is the relative frequency of class $y$ in the trining set. 

    \subsubsection{Implementation details}
    The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$. Given the continious nature of the selected dataset, the Gausian Naive Bayes Classifier was used. This means that the likelihood of the features is assumed to be Gaussian:
    $$P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma^2_y}}\cdot\exp{\left( -\frac{(x_i-\mu_y)^2}{2\sigma^2_y}\right)}$$

    The numerical implementation of the model computes the logarithm of the probabilities instead of the real probabilites to avoid underflow problems. This results in changing the $\prod$ by $\sum$.\\

    The only parameter the model has is \texttt{var\_smoothing} which is the portion of the largest variance of all features that is added to variances for calculation stability. Several models were fitter for different values for this parameter but the obtained results were very similar so the default value was used in the final model.
    % Reference https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB
1
   
    \subsection{\textcolor{red}{Support Vector Machines}}
    
    \subsection{\textcolor{red}{Comparison framework}}
    \label{sec:comparison}
    
    % GUIDANCE: Explain how you will compare the methods:
    
    % (1) Which metrics for comparison?
    % (2) How do you assess statistical significance of differences?
    % (3) What dimensions of comparison? (accuracy, interpretability, training time, robustness)
    % (4) Fairness of comparison: same data, same preprocessing, same tuning effort?
    
    [Your comparison framework here]
    
    % PROJECT-SPECIFIC NOTES:
    
    % - Project 1: Compare discriminative vs. generative approaches; discuss Bayesian interpretation
    % - Project 2: Compare neural vs. kernel methods; discuss primal-dual relationships if applicable
    % - Project 3: Compare advanced methods with simpler baselines from earlier parts; discuss theoretical connections (e.g., deep networks and NTK)
    
   \section{Discussion of results}
   \label{sec:discussion}

\subsection{Overall performance}
\label{sec:overall}

Table~\ref{tab:results} reports the test-set performance of the three models evaluated in this project: Logistic Regression, Linear SVM, and Gaussian Naive Bayes. All models achieve strong predictive accuracy, which is consistent with the low noise level and the separability observed during exploratory analysis. Logistic Regression and Linear SVM reach almost perfect classification, with no misclassified observations in the test set. Gaussian Naive Bayes performs slightly worse, although still above 90\% accuracy, confirming that the independence assumption limits its ability to fully capture feature interactions in this dataset.\\


Confusion matrices for each classifier are reported in Figure~\ref{fig:confusion-all}, allowing a visual comparison of true vs. predicted labels and highlighting the types of errors made by each model. These matrices provide a more nuanced view than accuracy alone, especially in a balanced binary classification setting. All three methods achieve strong performance, with relatively few misclassifications and a balanced behaviour between false positives and false negatives.


\medskip

\begin{table}[H]
    \centering
    \caption{Performance comparison of the three models on the test set.}
    \label{tab:results}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Method} & \textbf{Accuracy} & \textbf{F1-score} & \textbf{AUC} & \textbf{Training time (s)} \\
        \midrule
        Logistic Regression & $0.90 \pm 0.01$ & $0.89 \pm 0.02$ & $0.90 \pm 0.03$ & 6.3 \\
        \textcolor{red}{Linear SVM} & \textcolor{red}{NA}& \textcolor{red}{NA}& \textcolor{red}{NA}& \textcolor{red}{NA}\\
       \textcolor{red}{Gaussian Naive Bayes}  & \textcolor{red}{NA} & \textcolor{red}{NA} & \textcolor{red}{NA} & \textcolor{red}{NA}\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/LogRegConfusion.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/NaiveBayesConfusion.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/SVMConfusion.png}
    \end{minipage}

    \caption{Comparison of confusion matrices across the three models.}
    \label{fig:confusion-all}
\end{figure}



From a computational perspective, all models train extremely quickly due to the small size of the dataset. Logistic Regression and Linear SVM require iterative optimization, while Naive Bayes relies on closed-form parameter estimation, making it the fastest method overall.


    
    \subsection{\textcolor{red}{Detailed analysis}}
    \label{sec:detailed}
 
    % GUIDANCE: Provide deeper analysis of results:
    
    % (1) Why did certain methods perform better/worse? Connect to theoretical properties and data characteristics
    % (2) Error analysis: which examples were misclassified? Any patterns?
    % (3) Sensitivity analysis: how sensitive are results to hyperparameters?
    % (4) Behavior during training: convergence plots, learning curves
    % (5) Interpretability: what did the models learn? Feature importance, decision boundaries?
    %
    % Use visualizations effectively (learning curves, confusion matrices, feature importance plots, etc.)
    % CRITICAL: Go beyond surface observations. Provide insightful analysis connecting theory to practice.
    
    [Your detailed analysis here with insightful interpretation]
    
    % EXAMPLE FIGURE (adapt):
    % \begin{figure}[ht]
        % \centering
        % \includegraphics[width=0.8\textwidth]{figures/learning_curves.pdf}
        % \caption{Learning curves showing training and validation accuracy vs. training set size. Both methods show good generalization with small gap between training and validation curves. Method 2 benefits more from additional data, consistent with its higher capacity.}
        % \label{fig:learning}
        % \end{figure}

    \subsubsection{Naive Bayes}
The model was fitted without using the \texttt{points} feature.
Figure \ref{tab:NB_conf_mat} shows the confusion matrix obtained when the model was used to predict the test set. At a first glance one can see the model shows very good performance as it makes very few missclassifications. It is impresive how this level of accuracy is achived dispite not considering feature interactions.
The values oftained for Precission, Recal and F1-Score for the possitive class are: $0.9$, $0.89$ and $0.89$ respectively.\\

\begin{table}[ht]
 \centering
 \caption{Confussion matirx of Naive Bayes Classifier.}
 \label{tab:NB_conf_mat}
         \begin{tabular}{@{}lcccc@{}}
            \toprule
            & Predicted True & Predicted False \\
            \midrule
            Actual True & 206 & 18 \\
            Actual False & 20 & 156 \\
            \bottomrule
        \end{tabular}
\end{table}

In addition to checking its performance we also analyzed the distributions obtained for the two classes. Table \ref{tab:NB_dists} shows the estimated values for the mean and variance of the gausian distributions for $P(X|y)$ the model learned from the data. The model also provides the values for the prior probabilities of each class: True is $0.4394$ and False is $0.5606$.

    \begin{table}[ht]
        \centering
        \caption{Gausian distribution parameters ($\mu$,$\sigma^2$) learned by the model}
        \label{tab:NB_dists}
        \begin{tabular}{@{}lcccc@{}}
            \toprule
            \textbf{Class} & \textbf{Income} & \textbf{Credit Score} & \textbf{Loan Amount} & \textbf{Years Employed} \\
            \midrule
            True & ($100179.394$, $32005.074$) & ($702.168$, $116.514$) & ($22559.275$, $14869.246$) & ($22.060$, $11.055$) \\
            False & ($82527.957$, $34339.648$) & ($471.008$, $109.522$) & ($27209.9184$, $13461.882$) & ($19.266$, $12.108$) \\
            \bottomrule
        \end{tabular}
    \end{table}



    
    \subsection{Model selection and generalization error}
    \label{sec:final-model}
    
    % GUIDANCE: Select and justify your final model:
    % (1) Which method/configuration do you select as final model? Why?
    % (2) What is your estimate of generalization error? (on held-out test set)
    % (3) Confidence intervals or error bars?
    % (4) Is the model ready for deployment? Limitations?
    
    [Your final model selection and generalization error estimate here]
    
    \section{Conclusions}
    \label{sec:conclusions}
    
    % GUIDANCE: Synthesize your findings and reflect on the work.
    
    % (do NOT simply REPEAT the Abstract):
    %
    %    (1) Restate the problem and your approach (briefly)
    %    (2) What are the main findings? Which methods worked best? Why?
    %    (3) What insights did you gain about the problem or the methods?
    %    (4) Reflect on theoretical aspects
    %    (5) Limitations and future work
    %    (6) Reflect on what you learned
    
    \bibliographystyle{plain}
    \bibliography{references}
    
    % GUIDANCE ON REFERENCES:
    % - Use a .bib file for references (see references.bib template below)
    % - Cite all sources properly: papers, datasets, software packages
    % - Include at least a few references to establish context (related work, methods used)
    % - For datasets from repositories, cite the original source AND the repository
    % - For software packages, cite the paper if available, otherwise the documentation/URL
    % - For ChatGPT or similar tools, see: https://libguides.gettysburg.edu/citation/gen-ai
    
    % EXAMPLE .bib entries (create a separate references.bib file):
    % @article{author2020,
        %   author = {Author, A. and Author, B.},
        %   title = {Title of the paper},
        %   journal = {Journal Name},
        %   volume = {10},
        %   pages = {1--20},
        %   year = {2020}
        % }
    %
    % @misc{dataset2020,
        %   author = {Creator, C.},
        %   title = {Dataset name},
        %   year = {2020},
        %   howpublished = {\url{https://...}}
        % }
    
    \appendix
    
    \section{Additional results}
    \label{app:additional}
    
    % GUIDANCE: Use appendices for supplementary material that is important but would clutter
    % the main text. Examples:
    % - Additional experimental results or ablation studies
    % - Detailed derivations or proofs
    % - Additional visualizations
    % - Hyperparameter sensitivity analysis details
    % - Extended tables
    %
    % IMPORTANT: 
    % - Appendices DO NOT count toward the 10-page limit
    % - But don't use appendices to circumvent the page limit - main results should be in main text
    % - Reference appendix sections from main text when relevant
    
    [Your additional results here if needed]
    
    \section{Mathematical derivations}
    \label{app:derivations}
    
    % GUIDANCE: If you have detailed mathematical derivations that are important but too long
    % for the main text, include them here. Examples:
    
    % - Derivation of gradients for custom loss functions
    % - Proof of equivalence between formulations
    % - Derivation of dual problem from primal
    %
    % Use proper theorem/proof environments:
    
    % EXAMPLE (adapt or remove):
    \begin{lemma}
        State your lemma here with precise mathematical notation.
    \end{lemma}
    
    \begin{proof}
        Provide detailed proof here.
    \end{proof}
    
    % GUIDANCE ON MATHEMATICAL PRESENTATION:
    
    % - Use consistent notation throughout (define notation clearly in main text)
    % - Number important equations for reference
    % - Use proper mathematical environments (\begin{equation}, \begin{align}, etc.)
    % - Use theorem/lemma/proposition environments for formal statements
    % - Be rigorous but clear - explain the intuition alongside formal statements
            
    [Your derivations here if needed]
            
    \section{Implementation Details}
    \label{app:implementation}
            
            % GUIDANCE: Additional implementation details that might be useful but too technical for main text:
            
            % - Pseudocode for (additional) complex algorithms
            % - Architecture diagrams
            % - Detailed hyperparameter search results
            % - Convergence analysis details
            % - Important information about the code or the hardware used
            % - Remember full code goes in SEPARATE FILES
            % - ...
            
            [Your implementation details here if needed]
                       
        \end{document}
        
        % ============================================================================
        % GENERAL GUIDELINES FOR USING THIS TEMPLATE:
        %    see the pdf guide for additional information
        % ============================================================================
        %
        % 1. REMOVE ALL GUIDANCE COMMENTS before submitting (comments starting with %)
        %
        % 2. ADAPT STRUCTURE to your specific project - not all sections may be needed
        %    If your work is purely THEORETICAL, feel free to make any changes you need
        %
        % 3. MAINTAIN RIGOR: 
        %    - Use precise mathematical notation
        %    - State assumptions clearly
        %    - Justify all choices
        %    - Connect theory to practice
        %
        % 4. BE CONCISE but COMPLETE:
        %    - 10-page limit is strict for main text
        %    - Every sentence should add value
        %    - Use appendices for supplementary material
        %
        % 5. FIGURES AND TABLES:
        %    - Every figure/table must be referenced in text
        %    - Every figure/table must have informative caption
        %    - Explain what the reader should observe
        %
        % 6. REPRODUCIBILITY:
        %    - Provide enough detail that someone could reproduce your work
        %    - Document all hyperparameters, software versions, random seeds
        %
        % 7. PROOFREAD:
        %    - Check for grammar and spelling errors
        %    - Ensure consistent notation
        %    - Verify all cross-references work
        %    - Make sure bibliography is complete and properly formatteds
        %
        % ============================================================================