\documentclass[a4paper,10pt]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages for scientific writing
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref} % for smart cross-references

%% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%% Title information
\title{Project nº 1: Loan Approval Prediction with Linear and Bayesian Models\\
    Advanced Machine Learning (MDS)}

\author{Martin Tazón \and Daniel Weronski \and Martin de Peretti}

\date{Date: November 11, 2025}

\begin{document}
    
    \maketitle
    
    \begin{abstract}

%     GUIDANCE: The abstract should be a single paragraph (150-250 words) that concisely summarizes:

%     (1) The problem you are addressing and its motivation
We address a supervised binary classification problem: predicting loan approval from mixed tabular data containing both numerical and categorical features. Our objective is to evaluate three foundational methods from Part I of the course: a discriminative Generalized Linear Model (Logistic Regression), a Bayesian generative classifier (Naive Bayes), and a margin-based linear classifier (Linear SVM). \\

%     (2) The main methods/techniques you applied (from the corresponding course part)
% Do we care about the pipeline? Focus on the comparison of methods.
We construct a consistent and leakage-free preprocessing pipeline, including the removal of identifier variables, the encoding of categorical features, and the standardization of numerical attributes. All transformations are fitted exclusively on the training data within cross-validation folds to ensure methodological soundness. Performance will be evaluated using standard metrics for binary classification, including ROC-AUC, accuracy, and class-wise measures. \\

%     (3) The key results or findings
\textcolor{red}{TO DO (brief summary of empirical results and conclusions after completing the others parts) }
%     Write this LAST, after completing the rest of the report.
%     Example structure: "We address the problem of... using methods from Part [I/II/III] of the course, specifically... Our experimental results show that... We conclude that..." or ``we prove that ...''
        
    \end{abstract}
        
    \section{Introduction}
    \label{sec:introduction}

%    GUIDANCE: The introduction should provide context and motivation for your work. Include:
%     

%    (1) The problem you are addressing: what is it, why is it important or interesting?
Access to credit is one of the main factors influencing economic inclusion, yet loan approval decisions often rely on multiple socio-economic and financial criteria. 
%    (2) Brief background: what makes this problem challenging?
The presented dataset offers a greate example on how financial entityes rate their customers based on the data they have from them. There are two variables in the dataset that have been fabricated for this perpuse, "Credit Score" which is a financial riks score calculated from credit information by means of a standarized formula, and "Points". This last varible turns out to be the deciding factor used to determine the approval since it linearly separates the data in only one dimension. We will opt to use this variable or not depending on whather it has pedagogical meaning for the discused model. %NOT SURE ABOUT THIS%

%    (3) Your approach: what methods from the course are you applying? (mention Part I/II/III)
The dataset used in this work includes mostly numerical features, with only one ordinal features describing applicants’ profiles and their corresponding loan approval outcomes. 
Such data naturally involves uncertainty and feature correlations, making it suitable for exploring the concepts introduced in \emph{Part I} of the course, namely Bayesian reasoning, Generalized Linear Models (GLMs), and linear discriminant methods. For completeness we will fabricate a categorcal feature by classfinig the applicants by their work experience.\\

We focus on three complementary approaches:
\begin{itemize}
    \item the \textbf{Naive Bayes classifier}, which follows a Bayesian generative approach and assumes that features are conditionally independent given the target class;
    \item the \textbf{Logistic Regression model}, a Generalized Linear Model that directly estimates the probability of loan approval as a function of a linear combination of the features;
    \item the \textbf{Linear Support Vector Machine (SVM)}, which searches for a decision boundary that maximizes the margin between approved and rejected applicants.
\end{itemize}

These three methods represent different but complementary ways of learning from data. Comparing them within the same framework allows us to assess how these theoretical differences affect performance, interpretability, and robustness in a real-world classification task.\\

%    (4) Main contributions: what do you accomplish in this work?
The goal of this project is therefore to link theoretical understanding with practical application. 
We aim to analyze how each algorithm behaves on the same financial dataset, discuss their assumptions, and evaluate their ability to make reliable and interpretable predictions.\\

%    (5) Structure: briefly outline the rest of the document ("The rest of this report is organized as follows...")
The rest of this report is organized as follows. 
Section~\ref{sec:problem} formalizes the prediction problem. 
Section~\ref{sec:related} briefly reviews related works and previous approaches. % I would delte.
Section~\ref{sec:data} describes the dataset and preprocessing pipeline. 
Section~\ref{sec:methodology} presents the applied methods and experimental setup. 
Section~\ref{sec:discussion} discusses the results and insights. 
Finally, Section~\ref{sec:conclusions} summarizes the main findings and perspectives.

        
    \section{Problem statement}
    \label{sec:problem}
    \textcolor{red}{TO DO\\
    % GUIDANCE: 
    % (1) Clearly define what you are trying to accomplish
    % (2) Use mathematical notation appropriately but concisely.
    % (3) Make sure notation is clear
    %[Formal problem statement here]


    
    \section{Related Work}
    \label{sec:related}
    % GUIDANCE:   
    % (1) Brief description of relevant previous work on this problem or dataset
    % (2) How your work relates to or differs from previous approaches
    % (3) What you adopt or improve upon from related work
    % DO NOT copy/paste from papers - synthesize and cite properly!
    % EXAMPLE ALGORITHM IN PSEUDOCODE (adapt or remove, use for all algorithms throughout the work)
    % DO NOT display trivial non-informative pseudocode and make it match the notation used in the text
    \begin{algorithm}
        \caption{Algorithm name}
        \label{alg:example}
        \begin{algorithmic}[1]
            \REQUIRE Input data $X$, parameters $\theta_0$
            \ENSURE Optimized parameters $\theta^*$
            \STATE Initialize $\theta \leftarrow \theta_0$
            \WHILE{not converged}
            \STATE Compute gradient $g \leftarrow \nabla_\theta L(\theta)$
            \STATE Update $\theta \leftarrow \theta - \alpha g$
            \ENDWHILE
            \RETURN $\theta$
        \end{algorithmic}
    \end{algorithm}}
    
     %   [Your related work discussion here]



    
    \section{Data and Preprocessing}
    \label{sec:data}
    
    % GUIDANCE: Describe your data and all preprocessing steps, or omit this section entirely if not applicable. This section should be thorough because preprocessing significantly impacts results.
    This section describes the dataset used for the loan approval prediction task, as well as the preprocessing steps applied prior to model training. 
A careful understanding of the data and its preparation is essential to ensure the validity and interpretability of the results.

    
  \subsection{Data description}
\label{sec:data-description}

    % GUIDANCE: Describe the dataset(s) you are using, omit if not applicable.
    
%    Source: where did you obtain it? (cite properly)
%    Size: number of observations, features, classes (if classification)
%    Nature of features: continuous, categorical, mixed?
%    Domain: what does the data represent? What is the application context?
%    Known challenges: class imbalance, missing values, noise?
%    
%    A well-formatted table summarizing dataset characteristics is very useful here. \emph{See the project guide for more details}.
    
The dataset, entitled \emph{\href{https://www.kaggle.com/datasets/anishdevedward/loan-approval-dataset}{Loan Approval Dataset}}, was obtained from Kaggle and contains information about applicants and their corresponding loan approval status. Upon inspcting the details of the data we found it to be artificially generated using Faker Library.
It includes both numerical and categorical features that describe socio-economic and financial characteristics such as income, credit score, years employed, and loan amount. 
The target variable indicates whether a loan application was approved or not, making this a binary classification problem.\\

Two columns, \texttt{name} and \texttt{city}, were removed. 
The first one was unique for each applicant and therefore non-informative, while the second contained too few repetitions (maximum frequency of 4), making it irrelevant for the model. 
After removing these variables, the dataset was left with only continuous attributes.

No missing data or outliers were detected, which simplified preprocessing and ensured that no imputation or cleaning procedures were necessary, as one would expect from fabricated data. 


% Should update to new feature.
    \begin{table}[H]
        \centering
        \caption{Dataset characteristics after cleaning.}
        \label{tab:dataset}
        \begin{tabular}{@{}lll@{}}
            \toprule
            \textbf{Property} & \textbf{Variable} & \textbf{Value} \\
            \midrule
            Number of observations & $n$ & 2,000  \\
            Number of features & $d$ & 5 \\
            Feature types & continuous & Income / Credit score / Loan amount / Years employed / Points\\
            Number of classes & $C$ & 2 (Approved / Rejected) \\
            Class distribution & balanced & Approved : 56.05\% / Rejected : 43.95\% \\
            Missing values & percentage & 0\% \\
            Outliers detected & & None \\
            Source & & \href{https://www.kaggle.com/datasets/anishdevedward/loan-approval-dataset}{Kaggle} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
\subsection{Exploratory data analysis}
\label{sec:eda}

    % GUIDANCE: Describe your initial exploration of the data:
    
    % (3) Insights gained that inform preprocessing or modeling decisions
    %
    % IMPORTANT: Every figure must have a caption and be referenced and discussed in the text.
        
    % EXAMPLE FIGURE (remove this comment and adapt):

Before model development, a brief exploratory analysis was conducted to better understand the dataset’s structure and relationships between variables. 
The target variable (\texttt{loan\_approved}) is roughly balanced, which facilitates evaluation without the need for class reweighting or resampling techniques.\\

% (1) Visualization of key features or relationships
\paragraph{Feature distributions.}
The numerical features (\texttt{income}, \texttt{credit\_score}, \texttt{loan\_amount}, \texttt{years\_employed}, and \texttt{points}) were inspected to identify general trends and potential anomalies. 
Most variables exhibit mild right-skewness, which is typical of financial data (for instance, a small proportion of applicants with very high income or credit scores). 
No outliers or implausible values were observed.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/boxplotVariables.png}
    \caption{Distributions of numerical features by loan approval status. Applicants with higher income, credit score, and points are more likely to be approved.}
    \label{fig:boxplots}
\end{figure}

As shown in Figure~\ref{fig:boxplots}, loan approval is strongly associated with higher values of \texttt{income}, \texttt{credit\_score}, and \texttt{points}, confirming the relevance of financial stability indicators in credit assessment.
By contrast, \texttt{loan\_amount} and \texttt{years\_employed} display weaker class separation, suggesting they contribute less to the predictive task.\\
The feature \texttt{points} is special since the classes can be linearly separated by only looking at this variable, suggesting that this is an indicator computed by the financial entity to decide weather a loand should be accepted or not.

    % (2) Identification of patterns, outliers, correlations
\paragraph{Correlation analysis.}
To assess potential redundancy and feature interdependence, the Pearson correlation matrix was computed for all numerical variables (Figure~\ref{fig:corr}). 
The analysis reveals a strong positive correlation ($r \approx 0.74$) between \texttt{credit\_score} and \texttt{points}, and a moderate one ($r \approx 0.45$) between \texttt{income} and \texttt{points}. 
These relationships are expected given the shared financial nature of the indicators and it is consistent with  the theory of the \texttt{points} feature being a combination of the other metric to create a final de matric upon wich the entity takes decisions.
Importantly, the correlation values remain below critical thresholds, indicating no severe multicollinearity that would compromise model interpretability or stability.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/correlationMatrix.png}
    \caption{Correlation matrix of numerical features. A strong relation is observed between credit score and points.}
    \label{fig:corr}
\end{figure}

Overall, the exploratory analysis confirms that the dataset is clean, balanced, and well-suited for evaluating probabilistic and linear classification methods.
However, two decisions where taken at this stage:
\begin{enumerate}
    \item Since the inclussion of the feature \texttt{points} into the models may lead to perfect scores when those models can make use of the fact that the classes are linearly separable along this feature. The project aims to discuss the effect of leaving or excluding the vairalbe for each modeling approach.
    \item Given that the dataset lacks categorical variables, a new variable was included to showcase the ability of the studied machine learning models to account for this kind of variables. This new varible was a boolean representing those applicants with less than 10 years of experience.\\
\end{enumerate}
The insights gained here, particularly the correlation between key financial features, will be directly leveraged in the modeling phase to interpret each algorithm’s behavior and hopefully find out how the feature \texttt{points} was computed.


    \subsection{Preprocessing steps}
    \label{sec:preprocessing}
    
    % GUIDANCE: Document ALL preprocessing steps in detail. For each step, explain:
    
    % (1) WHAT you did
    % (2) WHY you did it (justify based on data characteristics or method requirements)
    % (3) HOW you did it (be specific about techniques used)
    %
    % Common preprocessing steps to address (as applicable):
    % - Missing value treatment: deletion, imputation (mean, median, model-based)? Justify choice.
    % - Outlier treatment: detection method? removal or transformation?
    % - Feature scaling/normalization: standardization, min-max? Why needed for your methods?
    % - Encoding categorical variables: one-hot, label encoding, target encoding? Why?
    % - Feature selection: filter, wrapper, embedded methods? Criteria used?
    % - Feature extraction: PCA, domain-specific transformations?
    % - Class imbalance: over/undersampling, class weights, posterior probabilities?




The preprocessing stage aimed to ensure data consistency and compliance with the theoretical assumptions of the three models under study: Naive Bayes, Logistic Regression, and Linear SVM. Although the dataset was already clean, a few key steps were necessary to prepare it for modeling.\\

The variables \texttt{name} and \texttt{city} were first removed. The former uniquely identifies each applicant and carries no predictive information, while the latter exhibits very low repetition (maximum frequency of four). Retaining these variables could introduce unnecessary noise and reduce the generalization capacity of the models. After this step, all remaining features were numerical, simplifying subsequent processing.\\

A systematic verification confirmed that the dataset contained no missing or incoherent values. This is relevant since models such as Logistic Regression and SVM do not natively handle missing data. Visual inspection via boxplots and histograms revealed no outliers: all features fall within plausible financial ranges, indicating that no trimming or imputation was required.\\

Correlation analysis (see Figure~\ref{fig:corr}) showed limited multicollinearity among most variables, except for a strong correlation ($r = 0.74$) between \texttt{credit\_score} and \texttt{points}. This redundancy was deliberately retained to observe how each learning paradigm behaves when predictors are dependent: it challenges the conditional independence assumption of Naive Bayes, may slightly affect coefficient interpretability in Logistic Regression, and has minimal impact on the Linear SVM’s separating margin. Preserving both features therefore enriches the comparative analysis between generative and discriminative methods.\\

Since all features were numeric but on different scales (e.g., \texttt{income} in thousands versus \texttt{credit\_score} in hundreds), a normalization step was necessary. Standardization was performed using \texttt{StandardScaler} from \texttt{scikit-learn}, transforming each feature $x_i$ as:
\[
x_i' = \frac{x_i - \mu_i}{\sigma_i}
\]
where $\mu_i$ and $\sigma_i$ are the mean and standard deviation estimated on the training set. This transformation centers the data around zero with unit variance, preventing large-scale features from dominating the optimization. It stabilizes learning for Logistic Regression and SVM and aligns feature variances with the probabilistic assumptions of Gaussian Naive Bayes.\\

To evaluate the effect of scaling, we visualized feature distributions before and after standardization (Figure~\ref{fig:scaling}). For instance, \texttt{income} originally exhibited a large range and mild right-skewness typical of financial data. After standardization, its distribution is re-centered and rescaled, preserving its overall shape and relative ordering. This confirms that the transformation harmonizes feature magnitudes without distorting their underlying structure, ensuring that all dimensions contribute comparably to model training.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/scaling.png}
    \caption{Effect of standardization on two numerical features. 
    The distributions are centered around zero and rescaled to unit variance, 
    preserving the shape of the data while ensuring comparability across features.}
    \label{fig:scaling}
\end{figure}

Lastly, the new metric \texttt{plus\_10y\_exp} was created by assigning 0 to instances where $years\_employed<10$ and 1 otherwise.


Overall, preprocessing primarily involved removing redundant variables and applying standardization. These steps yielded a consistent and theoretically sound dataset suitable for rigorous comparison across the three selected learning algorithms.


    \section{Methodology}
    \label{sec:methodology}
    
    % GUIDANCE: This is typically the longest section (2-3 pages). Describe all methods you applied
    % from the corresponding course part. For each method:
    
    % (1) Brief description: what is the method and its key properties?
    % (2) Why chosen: justify based on problem characteristics and theoretical properties
    % (3) Mathematical formulation: present the key equations precisely
    % (4) Implementation details: hyperparameters, optimization algorithm, convergence criteria
    % (5) Theoretical properties: discuss relevant theoretical results (e.g., universal approximation,
    %     consistency, convergence guarantees, computational complexity)
    %
    % Balance completeness with conciseness - you don't need to re-derive results from class, but you must demonstrate understanding.
    
    \subsection{Experimental protocol}
    \label{sec:protocol}
    
    % GUIDANCE: Describe your overall experimental setup BEFORE describing individual methods:
    %
    % (1) Data splitting: training/validation/test split? Cross-validation? Ratios?
    % (2) Performance metrics: which metrics and why? (accuracy, F1, AUC, RMSE, etc.)
    % (3) Hyperparameter tuning: method used (grid search, random search)? Search space?
    % (4) Statistical significance: how do you assess if differences are significant?
    % (5) Computational environment: hardware, software versions (for reproducibility)
    %
    % CRITICAL (resampling): Explain how you avoid data leakage (preprocessing fit on training set only,
    % applied to validation/test sets). This is essential for methodological rigor.

    
    [Your experimental protocol here]
    
    \subsection{Method 1: GML - Logistic Regression}
    \label{sec:method1}
    
    % GUIDANCE: For each method you apply, include:
    
    \subsubsection{Model formulation}
    
     % GUIDANCE: Present the mathematical formulation precisely. Use proper notation.
     %State the objective function, constraints, and key equations.
     
     For example, for a GLM, present the likelihood and regularized objective.
    
     %EXAMPLE (adapt to your method):
     The model minimizes the regularized empirical risk:
     \begin{equation}
         \hat{\theta} = \arg\min_{\theta \in \Theta} \left\{ \frac{1}{n}\sum_{i=1}^n \ell(y_i, f(x_i; \theta)) + \lambda R(\theta) \right\}
         \label{eq:objective}
         \end{equation}
     where $\ell$ is the loss function, $f(\cdot; \theta)$ is the prediction function, 
     $R(\theta)$ is the regularization term, and $\lambda > 0$ controls regularization strength. Recommended level of description:
     
     \begin{description}
         \item[one paragraph] for MLAs seen in class
         \item[half page] for MLAs not seen in class but mentioned
         \item[up to you] for MLAs not seen in class or own work
     \end{description}
    
    [Your mathematical formulation here]
    
    \subsubsection{Theoretical properties and justification}
    
    % GUIDANCE: Discuss relevant theoretical properties:
    
    % - Why is this method appropriate for your problem?
    % - What assumptions does it make? Are they satisfied in your data?
    % - What theoretical guarantees exist? (consistency, convergence, approximation error bounds)
    % - Computational complexity?
    % - Expected behavior on your problem type?
    %
    % This demonstrates theoretical rigor and deep understanding.
    
    [Your theoretical discussion here]
    
    \subsubsection{Implementation details}
    
    % GUIDANCE: Provide specific details needed for reproducibility:
    
    % - Software/library used (with version)
    % - Hyperparameters: which ones tuned? Search space? Final values?
    % - Optimization algorithm: which one? Convergence criteria? Learning rate schedule?
    % - Initialization: random? specific strategy?
    % - Any implementation choices or modifications
    %
    % Can be formatted as a table for clarity.
    
    [Your implementation details here]
    
    % EXAMPLE TABLE (adapt):
    \begin{table}[ht]
        \centering
        \caption{Hyperparameters for Method 1}
        \label{tab:hyper-method1}
        \begin{tabular}{@{}lll@{}}
            \toprule
            \textbf{Hyperparameter} & \textbf{Search space} & \textbf{Best value} \\
            \midrule
            Regularization $\lambda$ & $\{10^{-4}, 10^{-3}, \ldots, 10^2\}$ & $10^{-2}$ \\
            Learning rate & $\{0.001, 0.01, 0.1\}$ & $0.01$ \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    \subsection{Method 2: Naive Bayes Classifier}
    \label{sec:method2}

    This sections covers the use of the Naive Bayes Classifier when facing a binary classification problem. This modeling approach is unique compared to the others discussed in this project as it follows a genrative approach rather than a discriminative one.\\
    In broad terms one can think of discriminant models as trying to learn the separation between the classes by modeling $P(y|X)$, while on the other hand, generative algorithms try to learn $P(X|y)$ (and also $P(y)$). This implies that generative algorithms learn how the features are conditioned on the target class being 0 or 1.

    Naive Bayes are a fammily of supervised machine learining algorithms based on applying the Bayes' Theorem with the 'naive' assumption of conditional independance between every parir of features given the value of the target variable.
    These classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality at the cost of ignoring possible feature dependancies.

    Overall, Naive Bayes is known to be a decent and efficient classifier, however they are known to perform poorly as estimators, meaning that the probability outputs are not to be taken too seriously.
    
    \subsubsection{Model formulation} 
    
    Recall The Bayes Theorem:
    $$P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$
    
    The model aims to make a prediction on the class given the input data, wich can be written as:
    $$P(y|X)=P(y|x_1,...,x_n) = \frac{P(y)\cdot P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$

    The 'naive' assumption of conditional independence is expressed as:
    $$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_n) = P(x_i|y)$$
    And when applied into the Bayes Theorem $\forall i$ yelds:
    
    $$P(y|x_1,...,x_n)=\frac{P(y)\prod_{i=1}^n P(x_i|y)}{P(x_1,...,x_n)}$$

    Since $P(x_1,...,x_n)$ is constant given the input, we can simpligy and derive the following classification rule:
    $$ P(y|x,...,x_n) \propto P(y)\prod_{i=1}^nP(x_i|y) \implies \hat{y} = argmax_yP(y)\prod_{i=1}^nP(x_i|y) $$

    One can use Maximum A Posteriori estimation to estimate $P(x_i|y)$ and $P(y)$, where the later is the relative frequency of class $y$ in the trining set. 

    \subsubsection{Implementation details}
    The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$. Given the continious nature of the selected dataset, the Gausian Naive Bayes Classifier was used. This means that the likelihood of the features is assumed to be Gaussian:
    $$P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma^2_y}}\cdot\exp{\left( -\frac{(x_i-\mu_y)^2}{2\sigma^2_y}\right)}$$

    The numerical implementation of the model computes the logarithm of the probabilities instead of the real probabilites to avoid underflow problems. This results in changing the $\prod$ by $\sum$.

    The only parameter the model has is \texttt{var\_smoothing} which is the portion of the largest variance of all features that is added to variances for calculation stability. Several models were fitter for different values for this parameter but the obtained results were very similar so the default value was used in the final model.
    % Reference https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB

   
    \subsection{Method 3: Support Vector Machines}
    
    \subsection{Comparison framework}
    \label{sec:comparison}
    
    % GUIDANCE: Explain how you will compare the methods:
    
    % (1) Which metrics for comparison?
    % (2) How do you assess statistical significance of differences?
    % (3) What dimensions of comparison? (accuracy, interpretability, training time, robustness)
    % (4) Fairness of comparison: same data, same preprocessing, same tuning effort?
    
    [Your comparison framework here]
    
    % PROJECT-SPECIFIC NOTES:
    
    % - Project 1: Compare discriminative vs. generative approaches; discuss Bayesian interpretation
    % - Project 2: Compare neural vs. kernel methods; discuss primal-dual relationships if applicable
    % - Project 3: Compare advanced methods with simpler baselines from earlier parts; discuss theoretical connections (e.g., deep networks and NTK)
    
   \section{Discussion of results}
   \label{sec:discussion}
    
    % GUIDANCE: Present and interpret your results. This section should be evidence-based and analytical.
    
    % Structure: present results, then analyze/interpret them.
    % Length: 2-2.5 pages
    
    \subsection{Overall performance}
    \label{sec:overall}
    
    % GUIDANCE: Present the main results comparing all methods:
    %
    % (1) Performance metrics for all methods (table)
    % (2) Statistical significance of differences
    % (3) Which method performed best? By how much?
    % (4) Training times and computational costs
    %
    % IMPORTANT: Every table and figure must be discussed in the text. Don't just present
    % numbers - interpret them.
    
    [Your overall results here]
    
    % EXAMPLE TABLE (adapt):
    \begin{table}[ht]
        \centering
        \caption{Performance comparison of methods on test set. Results shown as mean $\pm$ standard deviation over 5-fold cross-validation. Bold indicates best performance.}
        \label{tab:results}
        \begin{tabular}{@{}lcccc@{}}
            \toprule
            \textbf{Method} & \textbf{Accuracy} & \textbf{F1-score} & \textbf{AUC} & \textbf{Training time (s)} \\
            \midrule
            Method 1 & $0.85 \pm 0.02$ & $0.83 \pm 0.03$ & $0.90 \pm 0.02$ & $15.3$ \\
            Method 2 & $\mathbf{0.88 \pm 0.02}$ & $\mathbf{0.86 \pm 0.02}$ & $\mathbf{0.92 \pm 0.01}$ & $127.8$ \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    \subsection{Detailed analysis}
    \label{sec:detailed}
 
    % GUIDANCE: Provide deeper analysis of results:
    
    % (1) Why did certain methods perform better/worse? Connect to theoretical properties and data characteristics
    % (2) Error analysis: which examples were misclassified? Any patterns?
    % (3) Sensitivity analysis: how sensitive are results to hyperparameters?
    % (4) Behavior during training: convergence plots, learning curves
    % (5) Interpretability: what did the models learn? Feature importance, decision boundaries?
    %
    % Use visualizations effectively (learning curves, confusion matrices, feature importance plots, etc.)
    % CRITICAL: Go beyond surface observations. Provide insightful analysis connecting theory to practice.
    
    [Your detailed analysis here with insightful interpretation]
    
    % EXAMPLE FIGURE (adapt):
    % \begin{figure}[ht]
        % \centering
        % \includegraphics[width=0.8\textwidth]{figures/learning_curves.pdf}
        % \caption{Learning curves showing training and validation accuracy vs. training set size. Both methods show good generalization with small gap between training and validation curves. Method 2 benefits more from additional data, consistent with its higher capacity.}
        % \label{fig:learning}
        % \end{figure}

    \subsubsection{Naive Bayes}
The model was fitted without using the \texttt{points} feature.
Figure \ref{tab:NB_conf_mat} shows the confusion matrix obtained when the model was used to predict the test set. At a first glance one can see the model shows very good performance as it makes very few missclassifications. It is impresive how this level of accuracy is achived dispite not considering feature interactions.
The values oftained for Precission, Recal and F1-Score for the possitive class are: $0.9$, $0.89$ and $0.89$ respectively.\\

\begin{table}[ht]
 \centering
 \caption{Confussion matirx of Naive Bayes Classifier.}
 \label{tab:NB_conf_mat}
         \begin{tabular}{@{}lcccc@{}}
            \toprule
            & Predicted True & Predicted False \\
            \midrule
            Actual True & 206 & 18 \\
            Actual False & 20 & 156 \\
            \bottomrule
        \end{tabular}
\end{table}

In addition to checking its performance we also analyzed the distributions obtained for the two classes. Table \ref{tab:NB_dists} shows the estimated values for the mean and variance of the gausian distributions for $P(X|y)$ the model learned from the data. The model also provides the values for the prior probabilities of each class: True is $0.4394$ and False is $0.5606$.

    \begin{table}[ht]
        \centering
        \caption{Gausian distribution parameters ($\mu$,$\sigma^2$) learned by the model}
        \label{tab:NB_dists}
        \begin{tabular}{@{}lcccc@{}}
            \toprule
            \textbf{Class} & \textbf{Income} & \textbf{Credit Score} & \textbf{Loan Amount} & \textbf{Years Employed} \\
            \midrule
            True & ($100179.394$, $32005.074$) & ($702.168$, $116.514$) & ($22559.275$, $14869.246$) & ($22.060$, $11.055$) \\
            False & ($82527.957$, $34339.648$) & ($471.008$, $109.522$) & ($27209.9184$, $13461.882$) & ($19.266$, $12.108$) \\
            \bottomrule
        \end{tabular}
    \end{table}



    
    \subsection{Model selection and generalization error}
    \label{sec:final-model}
    
    % GUIDANCE: Select and justify your final model:
    % (1) Which method/configuration do you select as final model? Why?
    % (2) What is your estimate of generalization error? (on held-out test set)
    % (3) Confidence intervals or error bars?
    % (4) Is the model ready for deployment? Limitations?
    
    [Your final model selection and generalization error estimate here]
    
    \section{Conclusions}
    \label{sec:conclusions}
    
    % GUIDANCE: Synthesize your findings and reflect on the work.
    
    % (do NOT simply REPEAT the Abstract):
    %
    %    (1) Restate the problem and your approach (briefly)
    %    (2) What are the main findings? Which methods worked best? Why?
    %    (3) What insights did you gain about the problem or the methods?
    %    (4) Reflect on theoretical aspects
    %    (5) Limitations and future work
    %    (6) Reflect on what you learned
    
    \bibliographystyle{plain}
    \bibliography{references}
    
    % GUIDANCE ON REFERENCES:
    % - Use a .bib file for references (see references.bib template below)
    % - Cite all sources properly: papers, datasets, software packages
    % - Include at least a few references to establish context (related work, methods used)
    % - For datasets from repositories, cite the original source AND the repository
    % - For software packages, cite the paper if available, otherwise the documentation/URL
    % - For ChatGPT or similar tools, see: https://libguides.gettysburg.edu/citation/gen-ai
    
    % EXAMPLE .bib entries (create a separate references.bib file):
    % @article{author2020,
        %   author = {Author, A. and Author, B.},
        %   title = {Title of the paper},
        %   journal = {Journal Name},
        %   volume = {10},
        %   pages = {1--20},
        %   year = {2020}
        % }
    %
    % @misc{dataset2020,
        %   author = {Creator, C.},
        %   title = {Dataset name},
        %   year = {2020},
        %   howpublished = {\url{https://...}}
        % }
    
    \appendix
    
    \section{Additional results}
    \label{app:additional}
    
    % GUIDANCE: Use appendices for supplementary material that is important but would clutter
    % the main text. Examples:
    % - Additional experimental results or ablation studies
    % - Detailed derivations or proofs
    % - Additional visualizations
    % - Hyperparameter sensitivity analysis details
    % - Extended tables
    %
    % IMPORTANT: 
    % - Appendices DO NOT count toward the 10-page limit
    % - But don't use appendices to circumvent the page limit - main results should be in main text
    % - Reference appendix sections from main text when relevant
    
    [Your additional results here if needed]
    
    \section{Mathematical derivations}
    \label{app:derivations}
    
    % GUIDANCE: If you have detailed mathematical derivations that are important but too long
    % for the main text, include them here. Examples:
    
    % - Derivation of gradients for custom loss functions
    % - Proof of equivalence between formulations
    % - Derivation of dual problem from primal
    %
    % Use proper theorem/proof environments:
    
    % EXAMPLE (adapt or remove):
    \begin{lemma}
        State your lemma here with precise mathematical notation.
    \end{lemma}
    
    \begin{proof}
        Provide detailed proof here.
    \end{proof}
    
    % GUIDANCE ON MATHEMATICAL PRESENTATION:
    
    % - Use consistent notation throughout (define notation clearly in main text)
    % - Number important equations for reference
    % - Use proper mathematical environments (\begin{equation}, \begin{align}, etc.)
    % - Use theorem/lemma/proposition environments for formal statements
    % - Be rigorous but clear - explain the intuition alongside formal statements
            
    [Your derivations here if needed]
            
    \section{Implementation Details}
    \label{app:implementation}
            
            % GUIDANCE: Additional implementation details that might be useful but too technical for main text:
            
            % - Pseudocode for (additional) complex algorithms
            % - Architecture diagrams
            % - Detailed hyperparameter search results
            % - Convergence analysis details
            % - Important information about the code or the hardware used
            % - Remember full code goes in SEPARATE FILES
            % - ...
            
            [Your implementation details here if needed]
                       
        \end{document}
        
        % ============================================================================
        % GENERAL GUIDELINES FOR USING THIS TEMPLATE:
        %    see the pdf guide for additional information
        % ============================================================================
        %
        % 1. REMOVE ALL GUIDANCE COMMENTS before submitting (comments starting with %)
        %
        % 2. ADAPT STRUCTURE to your specific project - not all sections may be needed
        %    If your work is purely THEORETICAL, feel free to make any changes you need
        %
        % 3. MAINTAIN RIGOR: 
        %    - Use precise mathematical notation
        %    - State assumptions clearly
        %    - Justify all choices
        %    - Connect theory to practice
        %
        % 4. BE CONCISE but COMPLETE:
        %    - 10-page limit is strict for main text
        %    - Every sentence should add value
        %    - Use appendices for supplementary material
        %
        % 5. FIGURES AND TABLES:
        %    - Every figure/table must be referenced in text
        %    - Every figure/table must have informative caption
        %    - Explain what the reader should observe
        %
        % 6. REPRODUCIBILITY:
        %    - Provide enough detail that someone could reproduce your work
        %    - Document all hyperparameters, software versions, random seeds
        %
        % 7. PROOFREAD:
        %    - Check for grammar and spelling errors
        %    - Ensure consistent notation
        %    - Verify all cross-references work
        %    - Make sure bibliography is complete and properly formatteds
        %
        % ============================================================================