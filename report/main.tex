\documentclass[a4paper,10pt]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages for scientific writing
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref} % for smart cross-references

%% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%% Title information
\title{Project nº 1: Loan Approval Prediction with Linear and Bayesian Models\\
    Advanced Machine Learning (MDS)}

\author{Martin Tazón \and Daniel Weronski \and Martin de Peretti}

\date{Date: November 11, 2025}

\begin{document}
    
    \maketitle
    
    \begin{abstract}

%     GUIDANCE: The abstract should be a single paragraph (150-250 words) that concisely summarizes:

%     (1) The problem you are addressing and its motivation
We address a supervised binary classification problem: predicting loan approval from mixed tabular data containing both numerical and categorical features. Our objective is to evaluate three foundational methods from Part I of the course: a discriminative Generalized Linear Model (Logistic Regression), a Bayesian generative classifier (Naive Bayes), and a margin-based linear classifier (Linear SVM). \\

%     (2) The main methods/techniques you applied (from the corresponding course part)
% Do we care about the pipeline? Focus on the comparison of methods.
%We construct a consistent and leakage-free preprocessing pipeline, including the removal of identifier variables, the encoding of categorical features, and the standardization of numerical attributes. All transformations are fitted exclusively on the training data within cross-validation folds to ensure methodological soundness.\\

%     (3) The key results or findings
Our key finding centers on the `points' feature, which renders the problem linearly separable. When this feature is excluded to model the underlying financial data, the problem becomes non-linear. On this more complex task, our experimental results show that the RBF-Kernel SVM achieves the highest test accuracy (0.94), significantly outperforming both the Linear SVM and Logistic Regression (0.90), as well as the Gaussian Naive Bayes (0.88). This highlights the critical trade-off between model complexity and feature selection in this domain.

%\textcolor{red}{TO DO (brief summary of empirical results and conclusions after completing the others parts) }
%     Write this LAST, after completing the rest of the report.
%     Example structure: "We address the problem of... using methods from Part [I/II/III] of the course, specifically... Our experimental results show that... We conclude that..." or ``we prove that ...''
        
    \end{abstract}
        
    \section{Introduction}
    \label{sec:introduction}

%    GUIDANCE: The introduction should provide context and motivation for your work. Include:
%     

%    (1) The problem you are addressing: what is it, why is it important or interesting?
Access to credit is one of the main factors influencing economic inclusion, yet loan approval decisions often rely on multiple socio-economic and financial criteria. \\ 

%    (2) Brief background: what makes this problem challenging?
The presented dataset offers a great example of how financial entities rate their customers based on the data they have from them. There are two variables in the dataset that have been fabricated for this process, "Credit Score", which is a financial risks score calculated from credit information by means of a standardized formula, and "Points". This last variable turns out to be the deciding factor used to determine the approval since it linearly separates the data in only one dimension. We will opt to use this variable or not depending on rather it has pedagogical meaning for the discused model. %NOT SURE ABOUT THIS%

%    (3) Your approach: what methods from the course are you applying? (mention Part I/II/III)
The dataset used in this work includes mostly numerical features, with only one ordinal features describing applicants’ profiles and their corresponding loan approval outcomes. 
Such data naturally involves uncertainty and feature correlations, making it suitable for exploring the concepts introduced in \emph{Part I} of the course, namely Bayesian reasoning, Generalized Linear Models (GLMs), and linear discriminant methods. For completeness we will fabricate a categorical feature by classifying the applicants by their work experience.\\

We focus on three complementary approaches:
\begin{itemize}
    \item the \textbf{Naive Bayes classifier}, which follows a Bayesian generative approach and assumes that features are conditionally independent given the target class;
    \item the \textbf{Logistic Regression model}, a Generalized Linear Model that directly estimates the probability of loan approval as a function of a linear combination of the features;
    \item the \textbf{Support Vector Machine (SVM)}, which searches for a decision boundary that maximizes the margin between approved and rejected applicants.
\end{itemize}

These three methods represent different but complementary ways of learning from data. Comparing them within the same framework allows us to assess how these theoretical differences affect performance, interpretability, and robustness in a real-world classification task.\\

%    (4) Main contributions: what do you accomplish in this work?
The goal of this project is therefore to link theoretical understanding with practical application. 
We aim to analyze how each algorithm behaves on the same financial dataset, discuss their assumptions, and evaluate their ability to make reliable and interpretable predictions.\\

%    (5) Structure: briefly outline the rest of the document ("The rest of this report is organized as follows...")
The rest of this report is organized as follows. 
Section~\ref{sec:problem} formalizes the prediction problem. 
Section~\ref{sec:related} briefly reviews related works and previous approaches. % I would delte.
Section~\ref{sec:data} describes the dataset and preprocessing pipeline. 
Section~\ref{sec:methodology} presents the applied methods and experimental setup. 
Section~\ref{sec:discussion} discusses the results and insights. 
Finally, Section~\ref{sec:conclusions} summarizes the main findings and perspectives.

        
    \section{\textcolor{red}{Problem statement}}
    \label{sec:problem}
    % GUIDANCE: 
    % (1) Clearly define what you are trying to accomplish
    % (2) Use mathematical notation appropriately but concisely.
    % (3) Make sure notation is clear
    %[Formal problem statement here]

    We can formalize the task as a supervised binary classification problem.
Let our dataset be $D = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$, where $n=2000$ is the total number of applicants.
Each applicant $i$ is represented by a feature vector $\mathbf{x}_i \in \mathbb{R}^d$, where $d=5$ (as described in \Cref{tab:dataset}). The features $\mathbf{x} = (\text{income}, \text{credit\_score}, \text{loan\_amount}, \text{years\_employed}, \text{points})$ are all continuous.
The corresponding label $y_i \in \{0, 1\}$ represents the loan approval status, where $y=1$ signifies `Approved' and $y=0$ signifies `Rejected'.

The primary objective is to learn a classification function (or hypothesis) $f: \mathbb{R}^d \to \{0, 1\}$ that minimizes the generalization error. This error is the expected 0-1 loss on new, unseen data, which we estimate using a held-out test set $D_{test} \subset D$:
\[
\mathcal{E}(f) = \frac{1}{|D_{test}|} \sum_{(\mathbf{x}, y) \in D_{test}} \mathbb{I}(f(\mathbf{x}) \neq y)
\]
where $\mathbb{I}(\cdot)$ is the indicator function.
A secondary objective is to compare the performance of different model families (generative vs. discriminative, linear vs. non-linear) on this specific task, particularly analyzing the impact of including or excluding the highly predictive `points` feature.


    
    \section{\textcolor{red}{Related Work}}
    \label{sec:related}
    % GUIDANCE:   
    % (1) Brief description of relevant previous work on this problem or dataset
    % (2) How your work relates to or differs from previous approaches
    % (3) What you adopt or improve upon from related work
    % DO NOT copy/paste from papers - synthesize and cite properly!
    % EXAMPLE ALGORITHM IN PSEUDOCODE (adapt or remove, use for all algorithms throughout the work)
    % DO NOT display trivial non-informative pseudocode and make it match the notation used in the text
    \begin{algorithm}
        \caption{Algorithm name}
        \label{alg:example}
        \begin{algorithmic}[1]
            \REQUIRE Input data $X$, parameters $\theta_0$
            \ENSURE Optimized parameters $\theta^*$
            \STATE Initialize $\theta \leftarrow \theta_0$
            \WHILE{not converged}
            \STATE Compute gradient $g \leftarrow \nabla_\theta L(\theta)$
            \STATE Update $\theta \leftarrow \theta - \alpha g$
            \ENDWHILE
            \RETURN $\theta$
        \end{algorithmic}
    \end{algorithm}
    
     %   [Your related work discussion here]



    
    \section{Data and Preprocessing}
    \label{sec:data}
    
    % GUIDANCE: Describe your data and all preprocessing steps, or omit this section entirely if not applicable. This section should be thorough because preprocessing significantly impacts results.
    This section describes the dataset used for the loan approval prediction task, as well as the preprocessing steps applied prior to model training. 
A careful understanding of the data and its preparation is essential to ensure the validity and interpretability of the results.

    
  \subsection{Data description}
\label{sec:data-description}

    % GUIDANCE: Describe the dataset(s) you are using, omit if not applicable.
    
%    Source: where did you obtain it? (cite properly)
%    Size: number of observations, features, classes (if classification)
%    Nature of features: continuous, categorical, mixed?
%    Domain: what does the data represent? What is the application context?
%    Known challenges: class imbalance, missing values, noise?
%    
%    A well-formatted table summarizing dataset characteristics is very useful here. \emph{See the project guide for more details}.
    
The dataset, entitled \emph{\href{https://www.kaggle.com/datasets/anishdevedward/loan-approval-dataset}{Loan Approval Dataset}}, was obtained from Kaggle and contains information about applicants and their corresponding loan approval status. Upon inspcting the details of the data we found it to be artificially generated using Faker Library.
It includes both numerical and categorical features that describe socio-economic and financial characteristics such as income, credit score, years employed, and loan amount. 
The target variable indicates whether a loan application was approved or not, making this a binary classification problem.\\

Two columns, \texttt{name} and \texttt{city}, were removed. 
The first one was unique for each applicant and therefore non-informative, while the second contained too few repetitions (maximum frequency of 4), making it irrelevant for the model. 
After removing these variables, the dataset was left with only continuous attributes.

No missing data or outliers were detected, which simplified preprocessing and ensured that no imputation or cleaning procedures were necessary, as one would expect from fabricated data. 


% Should update to new feature.
    \begin{table}[H]
        \centering
        \caption{Dataset characteristics after cleaning.}
        \label{tab:dataset}
        \begin{tabular}{@{}lll@{}}
            \toprule
            \textbf{Property} & \textbf{Variable} & \textbf{Value} \\
            \midrule
            Number of observations & $n$ & 2,000  \\
            Number of features & $d$ & 5 \\
            Feature types & continuous & Income / Credit score / Loan amount / Years employed / Points\\
            Number of classes & $C$ & 2 (Approved / Rejected) \\
            Class distribution & balanced & Approved : 56.05\% / Rejected : 43.95\% \\
            Missing values & percentage & 0\% \\
            Outliers detected & & None \\
            Source & & \href{https://www.kaggle.com/datasets/anishdevedward/loan-approval-dataset}{Kaggle} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
\subsection{Exploratory data analysis}
\label{sec:eda}

    % GUIDANCE: Describe your initial exploration of the data:
    
    % (3) Insights gained that inform preprocessing or modeling decisions
    %
    % IMPORTANT: Every figure must have a caption and be referenced and discussed in the text.
        
    % EXAMPLE FIGURE (remove this comment and adapt):

Before model development, a brief exploratory analysis was conducted to better understand the dataset’s structure and relationships between variables. 
The target variable (\texttt{loan\_approved}) is roughly balanced, which facilitates evaluation without the need for class reweighting or resampling techniques.\\

% (1) Visualization of key features or relationships
\paragraph{Feature distributions.}
The numerical features (\texttt{income}, \texttt{credit\_score}, \texttt{loan\_amount}, \texttt{years\_employed}, and \texttt{points}) were inspected to identify general trends and potential anomalies. 
Most variables exhibit mild right-skewness, which is typical of financial data (for instance, a small proportion of applicants with very high income or credit scores). 
No outliers or implausible values were observed.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/boxplotVariables.png}
    \caption{Distributions of numerical features by loan approval status. Applicants with higher income, credit score, and points are more likely to be approved.}
    \label{fig:boxplots}
\end{figure}

As shown in Figure~\ref{fig:boxplots}, loan approval is strongly associated with higher values of \texttt{income}, \texttt{credit\_score}, and \texttt{points}, confirming the relevance of financial stability indicators in credit assessment.
By contrast, \texttt{loan\_amount} and \texttt{years\_employed} display weaker class separation, suggesting they contribute less to the predictive task.\\
The feature \texttt{points} is special since the classes can be linearly separated by only looking at this variable, suggesting that this is an indicator computed by the financial entity to decide weather a loand should be accepted or not.

    % (2) Identification of patterns, outliers, correlations
\paragraph{Correlation analysis.}
To assess potential redundancy and feature interdependence, the Pearson correlation matrix was computed for all numerical variables (Figure~\ref{fig:corr}). 
The analysis reveals a strong positive correlation ($r \approx 0.74$) between \texttt{credit\_score} and \texttt{points}, and a moderate one ($r \approx 0.45$) between \texttt{income} and \texttt{points}. 
These relationships are expected given the shared financial nature of the indicators and it is consistent with  the theory of the \texttt{points} feature being a combination of the other metric to create a final de matric upon wich the entity takes decisions.
Importantly, the correlation values remain below critical thresholds, indicating no severe multicollinearity that would compromise model interpretability or stability.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/correlationMatrix.png}
    \caption{Correlation matrix of numerical features. A strong relation is observed between credit score and points.}
    \label{fig:corr}
\end{figure}

Overall, the exploratory analysis confirms that the dataset is clean, balanced, and well-suited for evaluating probabilistic and linear classification methods.
However, two decisions where taken at this stage:
\begin{enumerate}
    \item Since the inclussion of the feature \texttt{points} into the models may lead to perfect scores when those models can make use of the fact that the classes are linearly separable along this feature. The project aims to discuss the effect of leaving or excluding the vairalbe for each modeling approach.
    \item Given that the dataset lacks categorical variables, a new variable was included to showcase the ability of the studied machine learning models to account for this kind of variables. This new varible was a boolean representing those applicants with less than 10 years of experience.\\
\end{enumerate}
The insights gained here, particularly the correlation between key financial features, will be directly leveraged in the modeling phase to interpret each algorithm’s behavior and hopefully find out how the feature \texttt{points} was computed.


    \subsection{Preprocessing steps}
    \label{sec:preprocessing}
    
    % GUIDANCE: Document ALL preprocessing steps in detail. For each step, explain:
    
    % (1) WHAT you did
    % (2) WHY you did it (justify based on data characteristics or method requirements)
    % (3) HOW you did it (be specific about techniques used)
    %
    % Common preprocessing steps to address (as applicable):
    % - Missing value treatment: deletion, imputation (mean, median, model-based)? Justify choice.
    % - Outlier treatment: detection method? removal or transformation?
    % - Feature scaling/normalization: standardization, min-max? Why needed for your methods?
    % - Encoding categorical variables: one-hot, label encoding, target encoding? Why?
    % - Feature selection: filter, wrapper, embedded methods? Criteria used?
    % - Feature extraction: PCA, domain-specific transformations?
    % - Class imbalance: over/undersampling, class weights, posterior probabilities?




The preprocessing stage aimed to ensure data consistency and compliance with the theoretical assumptions of the three models under study: Naive Bayes, Logistic Regression, and Linear SVM. Although the dataset was already clean, a few key steps were necessary to prepare it for modeling.\\

The variables \texttt{name} and \texttt{city} were first removed. The former uniquely identifies each applicant and carries no predictive information, while the latter exhibits very low repetition (maximum frequency of four). Retaining these variables could introduce unnecessary noise and reduce the generalization capacity of the models. After this step, all remaining features were numerical, simplifying subsequent processing.\\

A systematic verification confirmed that the dataset contained no missing or incoherent values. This is relevant since models such as Logistic Regression and SVM do not natively handle missing data. Visual inspection via boxplots and histograms revealed no outliers: all features fall within plausible financial ranges, indicating that no trimming or imputation was required.\\

Correlation analysis (see Figure~\ref{fig:corr}) showed limited multicollinearity among most variables, except for a strong correlation ($r = 0.74$) between \texttt{credit\_score} and \texttt{points}. This redundancy was deliberately retained to observe how each learning paradigm behaves when predictors are dependent: it challenges the conditional independence assumption of Naive Bayes, may slightly affect coefficient interpretability in Logistic Regression, and has minimal impact on the Linear SVM’s separating margin. Preserving both features therefore enriches the comparative analysis between generative and discriminative methods.\\

Since all features were numeric but on different scales (e.g., \texttt{income} in thousands versus \texttt{credit\_score} in hundreds), a normalization step was necessary. Standardization was performed using \texttt{StandardScaler} from \texttt{scikit-learn}, transforming each feature $x_i$ as:
\[
x_i' = \frac{x_i - \mu_i}{\sigma_i}
\]
where $\mu_i$ and $\sigma_i$ are the mean and standard deviation estimated on the training set. This transformation centers the data around zero with unit variance, preventing large-scale features from dominating the optimization. It stabilizes learning for Logistic Regression and SVM and aligns feature variances with the probabilistic assumptions of Gaussian Naive Bayes.\\

To evaluate the effect of scaling, we visualized feature distributions before and after standardization (Figure~\ref{fig:scaling}). For instance, \texttt{income} originally exhibited a large range and mild right-skewness typical of financial data. After standardization, its distribution is re-centered and rescaled, preserving its overall shape and relative ordering. This confirms that the transformation harmonizes feature magnitudes without distorting their underlying structure, ensuring that all dimensions contribute comparably to model training.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/scaling.png}
    \caption{Effect of standardization on two numerical features. 
    The distributions are centered around zero and rescaled to unit variance, 
    preserving the shape of the data while ensuring comparability across features.}
    \label{fig:scaling}
\end{figure}

Lastly, the new metric \texttt{plus\_10y\_exp} was created by assigning 0 to instances where $years\_employed<10$ and 1 otherwise.\\

Overall, preprocessing primarily involved removing redundant variables and applying standardization. These steps yielded a consistent and theoretically sound dataset suitable for rigorous comparison across the three selected learning algorithms.


    \section{Methodology}
    \label{sec:methodology}
    
    % GUIDANCE: This is typically the longest section (2-3 pages). Describe all methods you applied
    % from the corresponding course part. For each method:
    
    % (1) Brief description: what is the method and its key properties?
    % (2) Why chosen: justify based on problem characteristics and theoretical properties
    % (3) Mathematical formulation: present the key equations precisely
    % (4) Implementation details: hyperparameters, optimization algorithm, convergence criteria
    % (5) Theoretical properties: discuss relevant theoretical results (e.g., universal approximation,
    %     consistency, convergence guarantees, computational complexity)
    %
    % Balance completeness with conciseness - you don't need to re-derive results from class, but you must demonstrate understanding.
    
%    \subsection{Experimental protocol}
%    \label{sec:protocol}
    
    % GUIDANCE: Describe your overall experimental setup BEFORE describing individual methods:
    %
    % (1) Data splitting: training/validation/test split? Cross-validation? Ratios?
    % (2) Performance metrics: which metrics and why? (accuracy, F1, AUC, RMSE, etc.)
    % (3) Hyperparameter tuning: method used (grid search, random search)? Search space?
    % (4) Statistical significance: how do you assess if differences are significant?
    % (5) Computational environment: hardware, software versions (for reproducibility)
    %
    % CRITICAL (resampling): Explain how you avoid data leakage (preprocessing fit on training set only,
    % applied to validation/test sets). This is essential for methodological rigor.


All experiments follow a rigorous and reproducible evaluation protocol designed to ensure fair comparison between the three classifiers studied in this work: Logistic Regression, Gaussian Naive Bayes, and Linear SVM. The dataset was first split into a training set (80\%) and a test set (20\%) using a fixed random seed, ensuring that all models are trained and evaluated on identical data partitions. To prevent data leakage, all preprocessing steps involving the computation of statistics, specifically feature standardization, were fitted exclusively on the training set and subsequently applied to the test set.\\

Model selection and hyperparameter tuning were performed through cross-validation on the training set. We used a 5-fold cross-validation strategy, in which the training data is partitioned into five equally sized folds: four folds are used for model fitting, and the remaining fold serves as a validation set. This process is repeated five times so that each fold is used once for validation. Cross-validation provides an unbiased estimate of generalization performance and reduces the variance associated with a single train/validation split.\\

For Logistic Regression and Linear SVM, the regularization parameter $C$ was tuned over a logarithmic grid in order to identify the best trade-off between model flexibility and generalization. For Gaussian Naive Bayes, the hyperparameter \texttt{var\_smoothing} was briefly examined across several orders of magnitude; as performance remained essentially unchanged, the default value was retained.\\

All models were evaluated using accuracy, precision, recall, and F1-score. These metrics are well suited for a balanced binary classification problem and allow a fair comparison between the generative (Naive Bayes) and discriminative (Logistic Regression, SVM) approaches.\\


All results reported in the next section correspond to performance on the held-out test set, which was not used at any stage of model selection. Experiments were conducted using Python 3.11 and \texttt{scikit-learn} 1.4, ensuring full reproducibility of the experimental environment.\\

    
    \subsection{GLM – Logistic Regression}
\label{sec:method1}

\subsubsection{Model formulation}

Logistic Regression is a Generalized Linear Model (GLM) designed for binary classification.  
Given input features $x \in \mathbb{R}^d$, the model assumes that the conditional probability of the positive class follows a logistic function:
\[
P(y = 1 \mid x; \theta) = \sigma(\theta^\top x)
\quad \text{with} \quad
\sigma(z)=\frac{1}{1+e^{-z}}.
\]

Let $\ell(\theta)$ denote the empirical logistic loss:
\[
\ell(\theta) = - \frac{1}{n} \sum_{i=1}^n 
\left( y_i \log \sigma(\theta^\top x_i)
+ (1-y_i)\log (1-\sigma(\theta^\top x_i)) \right).
\]

To prevent overfitting and improve numerical stability, an $\ell_2$ regularization term is added, leading to the following optimization problem:
\[
\hat{\theta}
=
\arg\min_{\theta \in \mathbb{R}^d}
\left\{
\ell(\theta) + 
\lambda \|\theta\|_2^2
\right\},
\]
where $\lambda = 1/C$ controls the strength of regularization (large $C$ = weak regularization).

The final classifier predicts:
\[
\hat{y} = 
\begin{cases}
1 & \text{if } \sigma(\theta^\top x) \geq 0.5, \\
0 & \text{otherwise}.
\end{cases}
\]


\subsubsection{Theoretical properties and justification}

Logistic Regression is a well-calibrated discriminative model that directly estimates $P(y \mid x)$ rather than modeling $P(x \mid y)$.  
It is particularly suitable for this dataset because:

\begin{itemize}
    \item all predictors are numerical and standardized, matching well the linear decision boundary assumption;
    \item the dataset size is moderate (2,000 observations), allowing fast and stable convex optimization;
    \item regularization mitigates multicollinearity effects, which is relevant given the strong correlation observed between \texttt{credit\_score} and \texttt{points};
    \item the model yields interpretable coefficients, enabling analysis of how features influence loan approval decisions.
\end{itemize}

The optimization problem is convex, guaranteeing the existence of a unique global optimum and robust convergence.  
Because of its probabilistic output and low variance, Logistic Regression is a strong baseline for binary financial decisions where interpretability and calibration are important.


\subsubsection{Implementation details}

The model was implemented using the \texttt{LogisticRegression} class from \texttt{scikit-learn}.  
All features were standardized prior to training.  
To select the regularization strength, the parameter $C$ was explored over a logarithmic grid:

\[
C \in \{10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^2\}.
\]

The solver \texttt{liblinear} was used due to the small number of features and its robustness on binary classification with $\ell_2$ penalty.

Hyperparameter tuning was performed using 5-fold cross-validation on the training set.  
No data leakage occurred: scaling was fitted exclusively on the training folds and applied to validation folds.

\begin{table}[ht]
    \centering
    \caption{Hyperparameters for Logistic Regression}
    \label{tab:hyper-logreg}
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Hyperparameter} & \textbf{Search space} & \textbf{Best value} \\
        \midrule
        Regularization strength $C$ & $\{10^{-3},10^{-2},10^{-1},1,10,10^2\}$ & $10^{-1}$ \\
        Penalty & $\{\ell_2\}$ & $\ell_2$ \\
        Solver & \texttt{liblinear}, \texttt{lbfgs} & \texttt{lbfgs} \\
        Max iterations & $\{100, 200, 500\}$ & 500 \\
        \bottomrule
    \end{tabular}
\end{table}

    \subsection{Naive Bayes Classifier}
    \label{sec:method2}

    This sections covers the use of the Naive Bayes Classifier when facing a binary classification problem. This modeling approach is unique compared to the others discussed in this project as it follows a genrative approach rather than a discriminative one.\\
    
    In broad terms one can think of discriminant models as trying to learn the separation between the classes by modeling $P(y|X)$, while on the other hand, generative algorithms try to learn $P(X|y)$ (and also $P(y)$). This implies that generative algorithms learn how the features are conditioned on the target class being 0 or 1.\\

    Naive Bayes are a fammily of supervised machine learining algorithms based on applying the Bayes' Theorem with the 'naive' assumption of conditional independance between every parir of features given the value of the target variable.\\ 
    
    These classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality at the cost of ignoring possible feature dependancies.\\

    Overall, Naive Bayes is known to be a decent and efficient classifier, however they are known to perform poorly as estimators, meaning that the probability outputs are not to be taken too seriously.\\
    
    \subsubsection{Model formulation} 
    
    Recall The Bayes Theorem:
    $$P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$
    
    The model aims to make a prediction on the class given the input data, wich can be written as:
    $$P(y|X)=P(y|x_1,...,x_n) = \frac{P(y)\cdot P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$

    The 'naive' assumption of conditional independence is expressed as:
    $$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_n) = P(x_i|y)$$
    And when applied into the Bayes Theorem $\forall i$ yelds:
    
    $$P(y|x_1,...,x_n)=\frac{P(y)\prod_{i=1}^n P(x_i|y)}{P(x_1,...,x_n)}$$

    Since $P(x_1,...,x_n)$ is constant given the input, we can simpligy and derive the following classification rule:
    $$ P(y|x,...,x_n) \propto P(y)\prod_{i=1}^nP(x_i|y) \implies \hat{y} = argmax_yP(y)\prod_{i=1}^nP(x_i|y) $$

    One can use Maximum A Posteriori estimation to estimate $P(x_i|y)$ and $P(y)$, where the later is the relative frequency of class $y$ in the trining set. 

    \subsubsection{Implementation details}
    The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$. Given the continious nature of the selected dataset, the Gausian Naive Bayes Classifier was used. This means that the likelihood of the features is assumed to be Gaussian:
    $$P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma^2_y}}\cdot\exp{\left( -\frac{(x_i-\mu_y)^2}{2\sigma^2_y}\right)}$$

    The numerical implementation of the model computes the logarithm of the probabilities instead of the real probabilites to avoid underflow problems. This results in changing the $\prod$ by $\sum$.\\

    The only parameter the model has is \texttt{var\_smoothing} which is the portion of the largest variance of all features that is added to variances for calculation stability. Several models were fitter for different values for this parameter but the obtained results were very similar so the default value was used in the final model.
    % Reference https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB
1
   
    \subsection{Support Vector Machines}
    \label{sec:method3}
    As a final ocmparison, we implement the Suport Vector Machine (SVM), a discriminative model that seeks to find an
    optimal separating hyperplane by maximizing the margin between classes. Unlike Logistic Regression, which models
    posterior probabilities, the SVM is a non-probabilistic linear classifier (in its basic form). We explore both the
    standard linear SVM and its non-linear extension using the kernel trick.

    \subsubsection{Model 1: Linear SVM}
    \label{sec:method3-linear}

    \paragraph{Model formulation}
    For a linearly separable problem, the SVM finds a unique hyperplane $(\mathbf{w}, b)$ that maximizes the margin $\frac{2}{\|\mathbf{w}\|}$. For the non-separable case, this is relaxed using slack variables $\xi_i \ge 0$. This leads to the soft-margin primal optimization problem:
    \[
    \min_{\mathbf{w}, b, \xi} \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i
    \quad \text{subject to} \quad
    y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0,
    \]
    where $C > 0$ is the regularization parameter. It controls the trade-off between maximizing the margin (a small $C$ encourages this) and minimizing the classification error on the training data (a large $C$ penalizes slack variables more heavily).

    The Linear SVM is a foundational linear classifier that, although not in Part I of the course serves as a direct comparison to Logistic Regression and is not bound to the linear separability condition. While both aim to find a linear decision boundary, they optimize different objective functions.

    The model was implemented using \texttt{scikit-learn}'s \texttt{SVC(kernel=`linear')} within a \texttt{Pipeline} that
    included \texttt{StandardScaler}. The \texttt{points} feature was excluded to ensure a fair comparison with the
    other models on the same feature subset. The regularization parameter $C$ was tuned via 5-fold cross-validation.

    \subsubsection{Model 2: RBF-Kernel SVM}
    \label{sec:method3-rbf}

    To extend the SVM to non-linear decision boundaries, we use the ``kernel trick.'' This avoids explicit feature mapping into a higher-dimensional space by replacing the dot product $\mathbf{x}_i^\top \mathbf{x}_j$ in the dual formulation with a kernel function $K(\mathbf{x}_i, \mathbf{x}_j)$. The decision function becomes:
    \[
    f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i K(\mathbf{x}, \mathbf{x}_i) + b\right)
    \]
    We specifically evaluated the Radial Basis Function (RBF) kernel, which is a popular, powerful choice:
    \[
    K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2), \quad \gamma > 0.
    \]
    Here, $\gamma$ is a new hyperparameter that defines the ``width'' or influence of each support vector. \\

    While the Linear SVM tests the linear separability of the feature subset, the RBF-Kernel SVM allows us to determine if a more complex, non-linear boundary can achieve better performance. This is particularly relevant since we removed the \texttt{points} feature, which (as noted in \Cref{sec:eda}) made the original dataset linearly separable.

    We used \texttt{SVC(kernel=`rbf')} and performed a grid search over both $C$ and $\gamma$ to find the optimal combination.

    \subsubsection{Hyperparameter tuning}
    The hyperparameters for both SVM models were tuned using 5-fold cross-validation. The search spaces and best-found parameters are summarized in Table~\ref{tab:hyper-svm} below.

    \begin{table}[H]
        \centering
        \caption{Hyperparameter search for SVM models, based on 5-fold CV.}
        \label{tab:hyper-svm}
        \begin{tabular}{@{}llll@{}}
            \toprule
            \textbf{Model} & \textbf{Hyperparameter} & \textbf{Search space} & \textbf{Best value} \\
            \midrule
            Linear SVM & Reg. strength $C$ & $\{0.01, 0.1, ..., 1000\}$ & $0.01$ \\
            \midrule
            RBF SVM & Reg. strength $C$ & $\{0.5, 1, ..., 5\}$ & $4.5$ \\
             & Kernel gamma $\gamma$ & $\{\text{'scale'}, \text{'auto'}, ..., 0.2\}$ & $0.2$ \\
            \bottomrule
        \end{tabular}
    \end{table}

    
    \subsection{Comparison framework}
    \label{sec:comparison}
    
    % GUIDANCE: Explain how you will compare the methods:
    
    % (1) Which metrics for comparison?
    % (2) How do you assess statistical significance of differences?
    % (3) What dimensions of comparison? (accuracy, interpretability, training time, robustness)
    % (4) Fairness of comparison: same data, same preprocessing, same tuning effort?
    
   To ensure a fair and rigorous evaluation, we compare the models across several dimensions, guided by the project requirements with ephasis on theoretical and empirical rigor.

\begin{itemize}
    \item \textbf{Predictive Performance:} The primary comparison is based on quantitative metrics calculated on the held-out test set. As shown in \Cref{tab:results}, we use Accuracy (our main estimate of generalization error), F1-score (Macro) (to account for any potential class imbalance in performance), and the AUC (CV) score (to provide a threshold-independent measure of class separability from the validation phase).

    \item \textbf{Model Paradigm:} We explicitly compare the two main learning paradigms:
    \begin{itemize}
        \item \textit{Generative} (Gaussian Naive Bayes) vs. \textit{Discriminative} (Logistic Regression, SVM).
        \item \textit{Linear} (Logistic Regression, Linear SVM) vs. \textit{Non-linear} (RBF-Kernel SVM).
    \end{itemize}

    \item \textbf{Feature Impact:} We analyze performance with and without the `points' feature to test the hypothesis of linear separability and understand the trade-offs of using this feature.

    \item \textbf{Interpretability vs. Complexity:} We discuss the trade-off between the highly interpretable coefficients of Logistic Regression and the ``black box'' nature of the high-performance RBF-Kernel SVM.

    \item \textbf{Training Efficiency:} We report the training time (as seen in \Cref{tab:results}) as a practical measure of computational cost, particularly in the context of hyperparameter tuning.
\end{itemize}
All models are trained on the same standardized 80\% training split and evaluated on the same 20\% test split to ensure all comparisons are valid.


    % PROJECT-SPECIFIC NOTES:
    
    % - Project 1: Compare discriminative vs. generative approaches; discuss Bayesian interpretation
    % - Project 2: Compare neural vs. kernel methods; discuss primal-dual relationships if applicable
    % - Project 3: Compare advanced methods with simpler baselines from earlier parts; discuss theoretical connections (e.g., deep networks and NTK)

\section{Discussion of results}
\label{sec:discussion}

\subsection{Overall performance}
\label{sec:overall}

Table~\ref{tab:results} reports the test-set performance of all evaluated models. For this comparison, all models were trained on the same feature subset, which excludes the \texttt{points} variable.

The results clearly differentiate the models. The RBF-Kernel SVM achieves the highest performance, with a test accuracy of $0.94$ and a corresponding F1-score of $0.94$. This suggests that the underlying relationship in this feature subset is non-linear. The Linear SVM and Logistic Regression models perform identically, both achieving $0.90$ accuracy. This is expected, as both are powerful linear classifiers optimizing similar (though not identical) objectives. The Gaussian Naive Bayes model performs the worst, though still achieving a respectable $0.88$ accuracy. This is likely due to its strict feature independence assumption being violated, as noted in the correlation analysis (\Cref{fig:corr}).

\begin{table}[H]
    \centering
    \caption{Performance comparison of the models on the test set (trained without the `points' feature).}
    \label{tab:results}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Method} & \textbf{Accuracy} & \textbf{F1-score (Macro)} & \textbf{AUC (CV)} & \textbf{Training time (s)} \\
        \midrule
        Logistic Regression & $0.90$ & $0.90$ & $0.97$ & $3.8$ \\
        Gaussian Naive Bayes & $0.88$ & $0.88$ & $0.95$ & 0.05 \\
        Linear SVM & $0.90$ & $0.90$ & $0.97$ & $3.1$ \\
        RBF-Kernel SVM & $\mathbf{0.94}$ & $\mathbf{0.94}$ & $\mathbf{0.99}$ & $4.2$4.2$ \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/LogRegConfusion.png}
        \label{fig:conf-logreg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/NaiveBayesConfusion.png}
        \label{fig:conf-nb}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        % This is the file your log reported as missing.
        % I am assuming it should be the RBF SVM, which was the best.
        % Please save the RBF plot as 'img/SVMConfusion.png'
        \includegraphics[width=\linewidth]{img/SVMConfusion.png}
        \label{fig:conf-svm-rbf}
    \end{minipage}

    \caption{Comparison of confusion matrices (Test Set). From left to right: Logistic Regression, Gaussian Naive Bayes, and RBF-Kernel SVM. The RBF SVM (right) shows the best performance.}
    \label{fig:confusion-all}
\end{figure}

\subsection{Detailed analysis}
\label{sec:detailed}

\subsubsection{Logistic Regression}
As shown in \Cref{tab:hyper-logreg}, the best regularization strength was $C=0.1$, indicating a preference for a relatively high degree of regularization. The model's coefficients (not shown) confirmed that \texttt{credit\_score} and \texttt{income} were the most significant positive predictors. Its performance was solid, but it was unable to capture the non-linearities that the RBF SVM exploited.

\subsubsection{Naive Bayes}
The model was fitted without using the \texttt{points} feature.
Figure~\Cref{fig:conf-nb} shows the confusion matrix obtained when the model was used to predict the test set. At a first glance one can see the model shows good performance.
The values obtained for Precision, Recall and F1-Score for the positive class are: $0.90$, $0.89$ and $0.89$ respectively.\\

In addition to checking its performance we also analyzed the distributions obtained for the two classes. \Cref{tab:NB_dists} shows the estimated values for the mean and variance of the Gaussian distributions for $P(X|y)$ the model learned from the data. The model also provides the values for the prior probabilities of each class: True is $0.4394$ and False is $0.5606$.

    \begin{table}[ht]
        \centering
        \caption{Gaussian distribution parameters ($\mu$,$\sigma^2$) learned by the model}
        \label{tab:NB_dists}
        \begin{tabular}{@{}lcccc@{}}
            \toprule
            \textbf{Class} & \textbf{Income} & \textbf{Credit Score} & \textbf{Loan Amount} & \textbf{Years Employed} \\
            \midrule
            True & ($100179.394$, $32005.074$) & ($702.168$, $116.514$) & ($22559.275$, $14869.246$) & ($22.060$, $11.055$) \\
            False & ($82527.957$, $34339.648$) & ($471.008$, $109.522$) & ($27209.9184$, $13461.882$) & ($19.266$, $12.108$) \\
            \bottomrule
        \end{tabular}
    \end{table}

\subsubsection{Support Vector Machines}
The SVM analysis provided the most interesting insights.
\begin{itemize}
    \item \textbf{Linear SVM:} The cross-validation selected a $C$ value of $0.01$. This is a very small $C$, which corresponds to a \emph{very strong} regularization. The model is prioritizing a wide, simple margin over fitting the data points. This choice resulted in a test accuracy of $0.90$, identical to Logistic Regression. This low $C$ value strongly suggests that the data (without the \texttt{points} feature) is not well-separated by a line, so the model opts for a highly regularized, ``simple'' boundary.

    \item \textbf{RBF-Kernel SVM:} This model achieved the best performance. The grid search selected $C=4.5$ and $\gamma=0.2$. This combination is the opposite of the Linear SVM: a high $C$ ($4.5$) means the model is heavily penalized for misclassifying points, and a high $\gamma$ ($0.2$) means the RBF kernels are narrow, creating a very flexible and complex decision boundary.
\end{itemize}

The clear superiority of the RBF-SVM ($94\%$ acc.) over the Linear-SVM ($90\%$ acc.) on this feature subset is a key finding. It empirically demonstrates that while the full dataset (including \texttt{points}) was linearly separable, the task becomes non-linear when this ``decision'' feature is removed. The RBF kernel was able to capture this non-linearity, while the linear models were not.


\subsection{Model selection and generalization error}
\label{sec:final-model}

Based on the test set performance (\Cref{tab:results}), the \textbf{RBF-Kernel SVM} is the clear choice for the final model, assuming the non-linear decision boundary is acceptable. It achieved the highest accuracy ($0.94$) and F1-score ($0.94$).

If interpretability and a linear decision boundary are strictly required (as is common in financial applications), both \textbf{Logistic Regression} and the \textbf{Linear SVM} are excellent and near-identical alternatives, with a generalization accuracy of $0.90$. The Naive Bayes model, while fast, is demonstrably weaker on this task. \\

We estimate the generalization error of our final selected model (the RBF-Kernel SVM) using its performance of the held-out test set, whih was not used during training or model selection.

The model achieved a point estimage for accuracy of 0.94 on the 400 test samples. We calcuated a 95\% confidence interval for this estimate which is 0.917--0.963. This suggests that we can be 95\% confident that the pure performance of our model on new, unseen data would be between 91.7\% and 96.3\%.



    % GUIDANCE: Select and justify your final model:
    % (1) Which method/configuration do you select as final model? Why?
    % (2) What is your estimate of generalization error? (on held-out test set)
    % (3) Confidence intervals or error bars?
    % (4) Is the model ready for deployment? Limitations?
    

    \section{Conclusions}
    \label{sec:conclusions}
    
    % GUIDANCE: Synthesize your findings and reflect on the work.
    
    % (do NOT simply REPEAT the Abstract):
    %
    %    (1) Restate the problem and your approach (briefly)
    %    (2) What are the main findings? Which methods worked best? Why?
    %    (3) What insights did you gain about the problem or the methods?
    %    (4) Reflect on theoretical aspects
    %    (5) Limitations and future work
    %    (6) Reflect on what you learned

    In this project, we successfully applied and evaluated three foundational classifiers --- Logistic Regression, Naive Bayes, and Support Vector Machines --- to a loan approval prediction task.

Our main finding is the critical impact of the `points' feature. The exploratory analysis showed this single feature linearly separates the data, and its inclusion would have led to perfect, but trivial, classification. By removing it, we uncovered a more complex and realistic problem.

The experimental results on this harder task clearly demonstrated the trade-offs between the models. The RBF-Kernel SVM achieved the highest test accuracy (0.94), confirming that the underlying relationship between the remaining financial features is non-linear. The linear models (Logistic Regression and Linear SVM) performed identically (0.90 accuracy), unable to capture this non-linearity. The Gaussian Naive Bayes model was the weakest (0.88 accuracy), likely due to its strong independence assumption being violated, as shown in the correlation matrix.

This project provided a practical link between theory and application. We demonstrated the importance of rigorous preprocessing and exploratory analysis in identifying data artifacts like the `points` feature. We confirmed the theoretical differences between generative and discriminative models, and empirically showed how the kernel trick allows SVMs to solve complex non-linear problems that linear models cannot.

For future work, a formal statistical test could be used to confirm if the RBF-SVM's 4\% performance gain is statistically significant. Furthermore, a more detailed analysis into the feature importance from the Logistic Regression model could provide valuable, interpretable insights for the financial domain, even if its predictive power is slightly lower.





    
    \bibliographystyle{plain}
%    \bibliography{references}
    
    % GUIDANCE ON REFERENCES:
    % - Use a .bib file for references (see references.bib template below)
    % - Cite all sources properly: papers, datasets, software packages
    % - Include at least a few references to establish context (related work, methods used)
    % - For datasets from repositories, cite the original source AND the repository
    % - For software packages, cite the paper if available, otherwise the documentation/URL
    % - For ChatGPT or similar tools, see: https://libguides.gettysburg.edu/citation/gen-ai
    
    % EXAMPLE .bib entries (create a separate references.bib file):
    % @article{author2020,
        %   author = {Author, A. and Author, B.},
        %   title = {Title of the paper},
        %   journal = {Journal Name},
        %   volume = {10},
        %   pages = {1--20},
        %   year = {2020}
        % }
    %
    % @misc{dataset2020,
        %   author = {Creator, C.},
        %   title = {Dataset name},
        %   year = {2020},
        %   howpublished = {\url{https://...}}
        % }
    
%    \appendix
    
%    \section{Additional results}
%    \label{app:additional}
    
    % GUIDANCE: Use appendices for supplementary material that is important but would clutter
    % the main text. Examples:
    % - Additional experimental results or ablation studies
    % - Detailed derivations or proofs
    % - Additional visualizations
    % - Hyperparameter sensitivity analysis details
    % - Extended tables
    %
    % IMPORTANT: 
    % - Appendices DO NOT count toward the 10-page limit
    % - But don't use appendices to circumvent the page limit - main results should be in main text
    % - Reference appendix sections from main text when relevant
    
%    [Your additional results here if needed]
    
%    \section{Mathematical derivations}
%    \label{app:derivations}
    
    % GUIDANCE: If you have detailed mathematical derivations that are important but too long
    % for the main text, include them here. Examples:
    
    % - Derivation of gradients for custom loss functions
    % - Proof of equivalence between formulations
    % - Derivation of dual problem from primal
    %
    % Use proper theorem/proof environments:
    
    % EXAMPLE (adapt or remove):
%    \begin{lemma}
%        State your lemma here with precise mathematical notation.
%    \end{lemma}
%
%    \begin{proof}
%        Provide detailed proof here.
%    \end{proof}
    
    % GUIDANCE ON MATHEMATICAL PRESENTATION:
    
    % - Use consistent notation throughout (define notation clearly in main text)
    % - Number important equations for reference
    % - Use proper mathematical environments (\begin{equation}, \begin{align}, etc.)
    % - Use theorem/lemma/proposition environments for formal statements
    % - Be rigorous but clear - explain the intuition alongside formal statements
            
%    [Your derivations here if needed]

%    \section{Implementation Details}
%    \label{app:implementation}

            % GUIDANCE: Additional implementation details that might be useful but too technical for main text:
            
            % - Pseudocode for (additional) complex algorithms
            % - Architecture diagrams
            % - Detailed hyperparameter search results
            % - Convergence analysis details
            % - Important information about the code or the hardware used
            % - Remember full code goes in SEPARATE FILES
            % - ...
            
%            [Your implementation details here if needed]
                       
        \end{document}
        
        % ============================================================================
        % GENERAL GUIDELINES FOR USING THIS TEMPLATE:
        %    see the pdf guide for additional information
        % ============================================================================
        %
        % 1. REMOVE ALL GUIDANCE COMMENTS before submitting (comments starting with %)
        %
        % 2. ADAPT STRUCTURE to your specific project - not all sections may be needed
        %    If your work is purely THEORETICAL, feel free to make any changes you need
        %
        % 3. MAINTAIN RIGOR: 
        %    - Use precise mathematical notation
        %    - State assumptions clearly
        %    - Justify all choices
        %    - Connect theory to practice
        %
        % 4. BE CONCISE but COMPLETE:
        %    - 10-page limit is strict for main text
        %    - Every sentence should add value
        %    - Use appendices for supplementary material
        %
        % 5. FIGURES AND TABLES:
        %    - Every figure/table must be referenced in text
        %    - Every figure/table must have informative caption
        %    - Explain what the reader should observe
        %
        % 6. REPRODUCIBILITY:
        %    - Provide enough detail that someone could reproduce your work
        %    - Document all hyperparameters, software versions, random seeds
        %
        % 7. PROOFREAD:
        %    - Check for grammar and spelling errors
        %    - Ensure consistent notation
        %    - Verify all cross-references work
        %    - Make sure bibliography is complete and properly formatteds
        %
        % ============================================================================